# ðŸ—„ï¸ NoSQL Internals: How Data Is Stored on Disk
## LSM Trees, SS Tables, WAL & Bloom Filters

> **Source:** [HLD Multi Master 11 â€“ YouTube](https://www.youtube.com/watch?v=qSMB6nUloNE)
> **Lecture Series:** Scaler HLD â€” Class 11
> **Last Updated:** February 2026
> **Goal:** Understand exactly how NoSQL databases store, retrieve, and manage data on disk using WAL, MemTable, SS Tables, LSM Trees, Bloom Filters, and Tombstones.

---

## ðŸ“‹ Table of Contents

1. [The Problem: Why SQL's Approach Doesn't Work for NoSQL](#1-the-problem-why-sqls-approach-doesnt-work-for-nosql)
2. [Brute Force Approach & Why It Fails](#2-brute-force-approach--why-it-fails)
3. [Write-Ahead Log (WAL)](#3-write-ahead-log-wal)
4. [In-Memory Hashmap (Key â†’ Offset)](#4-in-memory-hashmap-key--offset)
5. [MemTable: The In-Memory BBST](#5-memtable-the-in-memory-bbst)
6. [SS Tables: Sorted String Tables](#6-ss-tables-sorted-string-tables)
7. [LSM Tree: The Full Architecture](#7-lsm-tree-the-full-architecture)
8. [Sparse Index: The Memory-Efficient Index](#8-sparse-index-the-memory-efficient-index)
9. [Bloom Filters: Skipping Unnecessary Reads](#9-bloom-filters-skipping-unnecessary-reads)
10. [Tombstones: How Deletion Works](#10-tombstones-how-deletion-works)
11. [Full Read & Write Flow](#11-full-read--write-flow)
12. [Summary Cheat Sheet](#12-summary-cheat-sheet)

---

## 1. The Problem: Why SQL's Approach Doesn't Work for NoSQL

### SQL: Fixed-Size Rows â†’ Easy Updates

In SQL, every column has a known **data type** and every table has a **fixed schema**. This means you know the **exact size of every row** on disk before writing it.

```
SQL Table: users (id: BIGINT 8 bytes, name: VARCHAR(50))
  Row size = 8 + 50 = 58 bytes exactly

  [row 1: 58 bytes][row 2: 58 bytes][row 3: 58 bytes]...

  Update "Nikl" â†’ "Nikl Abra":
  Still fits in 50 bytes â†’ overwrite in place. âœ… Simple.
```

SQL indexes everything using **B+ Trees**:
- Each node of the B+ tree is exactly one disk block in size
- Height of tree = `log(N)` â†’ every read/write = `log(N)` disk seeks

### NoSQL: Variable-Size Data â†’ In-Place Update Breaks

In NoSQL (key-value stores, document stores), sizes vary:

```
Key   â†’ string of any length
Value â†’ string / JSON of any length

  Doc 10:  { "name": "Someone" }           â†’ 104 bytes
  Doc 30:  { "name": "N" }                 â†’ 84 bytes

  Update Doc 10 to: { "name": "Someone", "favorite_color": "red" }
  New size = 140 bytes > original 104 bytes â†’ OVERFLOW into Doc 30!
```

**Two bad options:**
1. Overwrite the next record â†’ **Data corruption**
2. Split document across disk â†’ **Fragmentation** â†’ multiple disk seeks

> **Disk seeks are brutally expensive** â€” on rotating disks they can take **100+ milliseconds**, vs sequential reads which are orders of magnitude faster. Even on SSDs, random I/O is ~100x slower than sequential I/O.

**The core problem:**
- NoSQL data has **variable sizes** â€” in-place updates are not safe
- NoSQL needs to be **optimized for heavy write loads** (unlike SQL)
- We cannot use B+ trees like SQL (writes would be slow + size changes break things)

---

## 2. Brute Force Approach & Why It Fails

**Approach: Just write key-value pairs to a flat file.**

```
file.db:
  [001 â†’ V Prasad]    â† 100 bytes, offset=0
  [002 â†’ N]           â† 50 bytes,  offset=100
  [100 â†’ Bit]         â† 60 bytes,  offset=150
  [060 â†’ Bishujit]    â† 40 bytes,  offset=210
  [030 â†’ Shashank]    â† 40 bytes,  offset=250
```

| Operation | Performance | Why                                       |
|-----------|-------------|-------------------------------------------|
| Write     | O(N)        | Must scan entire file to find & update entry |
| Read      | O(N)        | Must scan entire file linearly            |
| Update    | âŒ Dangerous | New value may be bigger â†’ overflows next entry |

**Key insight from the instructor:**

> *"The only safe solution for variable-size data: never update in-place. Just append at the end."*

**With append-only writes:**

```
  [001 â†’ V Prasad]
  [002 â†’ N]
  [100 â†’ Bit]
  [060 â†’ Bishujit]
  [030 â†’ Shashank]
  [060 â†’ Shashank]    â† New entry for key 060, appended
```

- Write becomes **O(1)** â€” just append to end (sequential write, no disk seek)
- But reads are still **O(N)** â€” linear scan from end to start to get latest value
- Duplicate entries pile up

---

## 3. Write-Ahead Log (WAL)

Every database (especially NoSQL) maintains a **Write-Ahead Log** â€” an **append-only file on disk** that records every change.

```
WAL File (on disk, append-only):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  SET key=001 value="V Prasad"
  SET key=002 value="N"
  SET key=060 value="Bishujit"
  SET key=060 value="Shashank"        â† update appended here
  DELETE key=030
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â† only appends, never modifies existing entries
```

### Why WAL Exists

| Use Case                  | How WAL Helps                                                  |
|---------------------------|----------------------------------------------------------------|
| **Crash Recovery**        | On restart, replay WAL to restore the latest state            |
| **Replication**           | Slaves ask: "What changed in the WAL?" and apply those changes|
| **Point-in-Time Recovery**| Keep N days of WAL â†’ restore DB to any past moment            |
| **DB Backups**            | Back up WAL snapshots; prune old entries after backup         |

**WAL properties:**
- Stored **on disk** (durable)
- **Immutable** â€” existing entries never modified; new events appended only
- Can be periodically purged once data is safely persisted
- Used by: Cassandra, PostgreSQL, SQLite, RocksDB, LevelDB

---

## 4. In-Memory Hashmap (Key â†’ Offset)

**Problem with WAL reads:** Still O(N) linear scan to find a key.

**Solution:** Maintain an **in-memory hashmap** that maps every key to its byte offset in the WAL file.

```
            WAL File (disk)                  Hashmap (RAM)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  offset=0:   001 â†’ V Prasad              001 â†’ offset 0
  offset=100: 002 â†’ N                     002 â†’ offset 100
  offset=150: 100 â†’ Bit                   100 â†’ offset 150
  offset=210: 060 â†’ Bishujit              060 â†’ offset 250   â† updated
  offset=250: 030 â†’ Shashank              030 â†’ offset 250
  offset=310: 060 â†’ Shashank         
```

```python
def read(key):
    offset = hashmap[key]           # O(1) RAM lookup
    buffer = file.read(n, offset)   # One disk seek to exact location
    return parse(buffer)            # Return value

def write(key, value):
    file.append(key, value)         # O(1) sequential write (no disk seek)
    hashmap[key] = file.current_size  # O(1) RAM update
```

- Reads: **O(1)** â€” hashmap lookup + single disk seek
- Writes: **O(1)** â€” append to disk + hashmap update

### Problem: Hashmap Gets Too Large

With **billions or trillions of keys**, the hashmap won't fit in RAM. Putting it on disk makes lookups slow again (random disk I/O).

> *"If you have trillions of keys and you put the hashmap on disk â€” life becomes slow again."*

We need a smarter approach. â†’ Enter the **MemTable**.

---

## 5. MemTable: The In-Memory BBST

**The Big Idea:** Instead of having the entire WAL be one huge file, split the WAL into **chunks**. Store the **latest (current) chunk in memory** rather than on disk.

This in-memory structure is called the **MemTable**.

### Why Not a Hashmap for the MemTable?

| Structure          | Read/Write | When Dumped to Disk        | Space            |
|--------------------|------------|----------------------------|------------------|
| **Hashmap**        | O(1)       | Loses hashmap property â†’ flat linear file (slow reads) | Wastes space (empty slots) |
| **Balanced BST (BBST)** | O(log N) | In-order traversal â†’ **automatically sorted** âœ… | Minimal overhead (just pointers) |

**MemTable = a Balanced Binary Search Tree (Red-Black Tree / Skip List) in memory.**

```
MemTable (in RAM, BBST):
           key=3
          /     \
       key=1   key=5
           \
          key=2

â†’ In-order traversal = [key=1, key=2, key=3, key=5] â†’ sorted!
```

### MemTable Write Flow

```
Write request: SET key=3 â†’ value=X

Step 1: Append to WAL file on disk (backup for crash recovery)
        WAL: ...| SET 3=X |â† small, only for recovery

Step 2: Update MemTable (BBST) in memory
        key=3 â†’ X (update or insert, no duplicates)

Step 3: Acknowledge to client âœ…
```

### MemTable as Read Cache

The MemTable acts as an **automatic read cache**:
- Recently written data is also often recently read
- If the key is in MemTable â†’ return from RAM, **no disk access**

```
Read key=3:
  â†’ Check MemTable â†’ FOUND â†’ return X âœ… (RAM speed, ultra fast)

Read key=100 (old data, not in MemTable):
  â†’ Check MemTable â†’ NOT FOUND â†’ look in SS Tables on disk
```

### What Happens When MemTable is Full?

When the MemTable exceeds a configured threshold (e.g., **100 MB**):

```
MemTable full â†’ flush to disk as a new SS Table file
             â†’ clear MemTable
             â†’ delete the WAL backup file (no longer needed)
             â†’ create new empty MemTable
```

Because the MemTable was a BBST, its in-order dump is **already sorted**.

---

## 6. SS Tables: Sorted String Tables

When the MemTable is flushed to disk, it creates a **Sorted String Table (SS Table)**.

> *"We call these files SS Tables â€” Sorted String Tables â€” because they store strings and they are sorted."*

### SS Table Properties

```
SS Table File (on disk, immutable, sorted):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  key=1 â†’ value=Y
  key=2 â†’ value=W
  key=3 â†’ value=X
  key=5 â†’ value=C
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  âœ… Sorted by key
  âœ… No duplicates within one file
  âœ… Immutable â€” never updated after creation
  âœ… Binary search possible (sorted!) 
  âŒ Can have duplicates across different SS Table files
```

### Multiple SS Tables Over Time

As more data is written, MemTable fills and flushes repeatedly:

```
Disk:
  1.sst  [key=1â†’A, key=2â†’B, key=3â†’C]               â† older
  2.sst  [key=1â†’X, key=4â†’D, key=5â†’C, key=2â†’W]      â† newer
  3.sst  [key=1â†’Y, key=2â†’W, key=3â†’X]               â† newest

MemTable (RAM):
  [key=2â†’X, key=3â†’Y, key=1â†’W]                       â† current
```

Observation: key=1 appears in **all three files** â€” inter-file duplicates exist but are handled by reading newest first.

### Background Compaction Process

A background process periodically merges older SS Tables into larger ones:

- Both files are sorted â†’ use **merge sort** algorithm â†’ result is also sorted
- Eliminate inter-file duplicate keys (keep only the most recent value)
- Result: fewer, larger SS Tables

```
Before compaction:
  1.sst  [1â†’A, 2â†’B, 3â†’C]
  2.sst  [1â†’X, 4â†’D, 5â†’C, 2â†’W]

After compaction (merge, keep latest):
  XL1.sst [1â†’X, 2â†’W, 3â†’C, 4â†’D, 5â†’C]   â† merged, no duplicates
```

> âš ï¸ **Compaction is critical to tune in production.** A poorly tuned compaction process directly impacts read and write performance. Tune: chunk size, compaction frequency, time of day (lowest traffic).

---

## 7. LSM Tree: The Full Architecture

Multiple levels of SS Tables, growing in size, form a **Log-Structured Merge Tree (LSM Tree)**.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  MemTable (RAM, BBST)   â”‚ â† all writes go here
                    â”‚  ~100 MB, sorted        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ flush when full
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      Level 0:      â”‚  SST1 SST2 SST3 SST4   â”‚ â† small, ~100 MB each
      (disk)        â”‚  sorted, may overlap    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ compaction (merge sort)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      Level 1:      â”‚    SST_A     SST_B      â”‚ â† larger, ~200-400 MB
      (disk)        â”‚    sorted, merged       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ compaction
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      Level 2:      â”‚         SST_XL          â”‚ â† largest, 1 GB+
      (disk)        â”‚    fully merged         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      WAL file (disk, separate):
                    â”‚ backup for MemTable     â”‚
                    â”‚ deleted after each flushâ”‚
```

**Write complexity: O(1)** â€” append to WAL + update MemTable (both sequential/in-memory)

**Read complexity: O(log N)** â€” check MemTable â†’ linear scan across small number of SS Tables, binary search inside each

> *"The number of SS tables is kept small by the compaction process â€” it's order log(N)."*

---

## 8. Sparse Index: The Memory-Efficient Index

**Problem:** The hashmap storing every key â†’ SS Table mapping is too large for billions of keys.

**Solution:** For each SS Table, maintain a **sparse index** in memory â€” store only the **first key of every 64 KB block**, not every key.

```
SS Table on disk (1 GB, sorted):

  Block 0 (0â€“64 KB):     key=0 ... key=100
  Block 1 (64â€“128 KB):   key=101 ... key=2010
  Block 2 (128â€“192 KB):  key=2011 ... key=5000
  ...
  Block 16383 (last):    key=... ... key=N

Sparse Index in memory (for this 1 GB SS Table):
  key=0    â†’ offset=0
  key=101  â†’ offset=65536
  key=2011 â†’ offset=131072
  ...

  Total index entries = 1 GB / 64 KB = 2^14 â‰ˆ 16,000
  If each key is 64 bytes: index size = 16,000 Ã— 64 = 1 MB only!
```

> *"If this SS Table is 1 GB, this sparse index is only 1 MB â€” a 1000x reduction!"*

### How Reading Works With Sparse Index

```
Read: key=300

1. Binary search the sparse index (in RAM):
   key=0 (offset=0) < key=300 < key=2011 (offset=131072)
   â†’ key=300 is in Block 1 starting at offset=65536

2. Read the 64 KB block from disk (ONE disk seek):
   Block 1: [key=101, key=150, key=200, key=300, ...]

3. Binary search within the 64 KB block (fast, in memory):
   â†’ Found: key=300 â†’ value=...

Total: 1 disk seek per SS Table checked.
```

---

## 9. Bloom Filters: Skipping Unnecessary Reads

**Problem:** If we search for a key that doesn't exist, we scan all SS Tables, check all sparse indexes, do disk seeks â€” and find nothing. Wasted work.

**Solution:** A **Bloom Filter** â€” a fixed-size bit array that tells you, *with very high probability*, whether a key was ever inserted.

### Bloom Filter Properties

```
Bloom Filter:
  âœ… If key IS present â†’ guaranteed to return TRUE (no false negatives)
  âœ… If key IS NOT present â†’ returns FALSE with high probability
  âš ï¸  May have false positives (says "exists" when it doesn't) â€” tunable
  âœ… Fixed size â€” does NOT grow as you insert more keys!
```

> *"In case of a set or hashmap, as you insert more data, the size will grow. In case of a bloom filter, the size does not change at all. That is magic."*

### How a Bloom Filter Works

```
Bloom filter = a bit array of size M, initially all zeros:
  [ 0 0 0 0 0 0 0 0 0 0 0 0 ]
    0 1 2 3 4 5 6 7 8 9 10 11

K = number of hash functions (e.g., K=3)
```

**Insert "Nikl":**
```
  h1("Nikl") = 2  â†’ set bit[2] = 1
  h2("Nikl") = 3  â†’ set bit[3] = 1
  h3("Nikl") = 9  â†’ set bit[9] = 1

  [ 0 0 1 1 0 0 0 0 0 1 0 0 ]
          â†‘ â†‘           â†‘
```

**Insert "Vishal":**
```
  h1("Vishal") = 2  â†’ bit[2] already 1
  h2("Vishal") = 5  â†’ set bit[5] = 1
  h3("Vishal") = 11 â†’ set bit[11] = 1

  [ 0 0 1 1 0 1 0 0 0 1 0 1 ]
```

**Check "V Prasad" (NOT inserted):** h("V Prasad") = {1, 3, 7}
```
  bit[1] = 0 â†’ DEFINITE NO â†’ "V Prasad" does not exist âœ… (no disk read needed!)
```

**Check "Abhishek" (NOT inserted):** h("Abhishek") = {3, 9, 11}
```
  bit[3]=1, bit[9]=1, bit[11]=1 â†’ All bits set!
  Bloom filter says: "MAYBE EXISTS"
  â†’ Have to search SS Tables â†’ not found â†’ false positive âš ï¸
```

### Bloom Filter Formula

```
P(false positive) â‰ˆ (1 - e^(-kÂ·n/m))^k

Where:
  k = number of hash functions
  n = number of keys inserted
  m = number of bits in filter

Example: m = 10 MB (80,000,000 bits), n = 1 billion keys, k = 7
  â†’ False positive rate â‰ˆ 1% (very rare)
  
vs. storing 1 billion keys in a hashmap: ~100 GB
    storing in a bloom filter: just 10 MB! (10,000x smaller)
```

### Real-World Use Case: CDN (Akamai / Cloudflare)

> *"70% of URLs on the web are only visited once. So CDNs cache a URL only after it has been visited twice."*

- To know if a URL was seen before â†’ need to store all seen URLs
- 1 trillion URLs Ã— 500 bytes/URL = **500 TB** of data if stored as a hashmap
- With Bloom Filter â†’ a few **GB** with acceptable false positive rate

---

## 10. Tombstones: How Deletion Works

**Problem:** You cannot delete data directly from SS Tables (they're immutable). You cannot delete from a Bloom Filter either (it never forgets).

**Solution: Tombstones** â€” a special marker value that means "this key has been deleted."

### Deletion as a Write

```
Delete: key=060

Step 1: Write a tombstone to MemTable:
        key=060 â†’ TOMBSTONE

Step 2: This tombstone value is flushed to an SS Table like any other write

Step 3: On read:
        key=060 found â†’ value = TOMBSTONE â†’ return "key not found"
```

### During Compaction

```
SS Table 1 (older):  key=060 â†’ "Bishujit"
SS Table 2 (newer):  key=060 â†’ TOMBSTONE

After compaction (keep latest):
  Result:            key=060 â†’ TOMBSTONE   â† old value removed

After even later compaction:
  If tombstone is old enough and no risk of confusion:
  â†’ tombstone itself is removed â†’ true physical deletion
```

### Why Tombstones Are Needed Even With Bloom Filters

- When key=060 was first written, it was inserted into the Bloom Filter
- The Bloom Filter **cannot delete** â€” it will always say "key=060 maybe exists"
- Without a tombstone: every read for key=060 would scan all SS Tables (wasted work)
- **With a tombstone**: the scan terminates early when the tombstone is found â€” efficient!

> *"Bloom filter never forgets. So the tombstone is what tells the system: 'yes the bloom filter says it's here, but it has actually been deleted.'"*

---

## 11. Full Read & Write Flow

### Write Flow

```mermaid
sequenceDiagram
    participant Client
    participant WAL as WAL File (Disk)
    participant Mem as MemTable (RAM, BBST)
    participant SST as SS Tables (Disk)

    Client->>WAL: 1. Append write (crash recovery backup)
    Client->>Mem: 2. Insert/update in BBST
    Mem-->>Client: 3. ACK âœ…

    Note over Mem: When MemTable full (~100 MB)...
    Mem->>SST: 4. Flush as new sorted SS Table
    SST-->>WAL: 5. Delete WAL backup (data safely on disk)
    Note over SST: Background: compact SS Tables regularly
```

### Read Flow

```mermaid
flowchart TD
    R[Read: key=X] --> M{Check MemTable?\nO(log N) in RAM}
    M -->|Found| RET[âœ… Return value\nno disk access]
    M -->|Not found| BF{Check Bloom Filter\nfor each SS Table}
    BF -->|Definite NO| SKIP[Skip this SS Table\nno disk read]
    BF -->|Maybe YES| SI[Binary search Sparse Index\nin memory]
    SI --> BLOCK[One disk seek\nâ†’ 64 KB block]
    BLOCK --> BS{Binary search\nwithin 64 KB block}
    BS -->|Found Tombstone| NULL[Return: key deleted]
    BS -->|Found value| RET
    BS -->|Not in this SST| BF
    SKIP --> BF
    BF -->|All SSTs checked| NOPE[Return: key not found]
```

### Performance Summary

| Operation | Complexity | Notes |
|-----------|-----------|-------|
| **Write** | O(1) | Append to WAL + MemTable insert |
| **Read (in MemTable)** | O(log N_mem) | In-memory BBST, extremely fast |
| **Read (in SS Tables)** | O(log N) | Linear over #SSTables Ã— binary search per SSTable |
| **Read (key absent)** | Near O(log N) | Bloom filter skips most SSTables |

---

## 12. Summary Cheat Sheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           NoSQL Storage Architecture â€” Quick Reference           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Component         â”‚ One-Line Summary                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WAL               â”‚ Append-only disk file for crash recovery,    â”‚
â”‚                   â”‚ replication, and point-in-time restore       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MemTable          â”‚ Balanced BST in RAM; absorbs all writes;     â”‚
â”‚                   â”‚ acts as automatic read cache; flushed to diskâ”‚
â”‚                   â”‚ when full â†’ becomes an SS Table              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SS Table          â”‚ Immutable, sorted file on disk; no duplicatesâ”‚
â”‚                   â”‚ within a file; flushed from MemTable         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LSM Tree          â”‚ Multi-level hierarchy of SS Tables; small    â”‚
â”‚                   â”‚ files at top, large merged files at bottom   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Compaction        â”‚ Background merge of SS Tables using merge    â”‚
â”‚                   â”‚ sort; removes duplicates, reduces file count â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sparse Index      â”‚ In-memory index: one entry per 64 KB block;  â”‚
â”‚                   â”‚ 1000x smaller than full key index; enables   â”‚
â”‚                   â”‚ fast binary search into SS Tables            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Bloom Filter      â”‚ Fixed-size bit array; no false negatives;    â”‚
â”‚                   â”‚ skips disk reads for absent keys;            â”‚
â”‚                   â”‚ 10 MB handles 1 billion keys at 1% FP rate  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tombstone         â”‚ Special marker value for deleted keys;       â”‚
â”‚                   â”‚ deletion = write(key, TOMBSTONE);            â”‚
â”‚                   â”‚ physically removed during compaction         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Databases That Use This Architecture

| Database        | LSM Tree | WAL | Bloom Filter | Notes                        |
|-----------------|----------|-----|--------------|------------------------------|
| **Cassandra**   | âœ…       | âœ…  | âœ…           | Compaction tuning is critical|
| **DynamoDB**    | âœ…       | âœ…  | âœ…           | AWS managed                  |
| **RocksDB**     | âœ…       | âœ…  | âœ…           | Used inside many DBs (MySQL) |
| **LevelDB**     | âœ…       | âœ…  | âœ…           | Google-built library         |
| **HBase**       | âœ…       | âœ…  | âœ…           | Hadoop ecosystem             |
| **ScyllaDB**    | âœ…       | âœ…  | âœ…           | Cassandra-compatible         |

---

> ðŸ“š **Related Topics:**
> - [SQL vs NoSQL Ultimate Guide](../SQL%20vs%20NoSQL/SQL_vs_NoSQL_Ultimate_Guide.md)
> - [CAP Theorem & Replication](../CAP%20Theorem%20%26%20Replication/)
> - **Next in Series:** Consensus Algorithms, Distributed Transactions
