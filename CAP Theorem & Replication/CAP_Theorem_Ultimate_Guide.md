# ðŸš€ CAP Theorem, PACELC & Replication â€” The Ultimate System Design Guide

> **Last Updated:** February 2026
> **Author:** System Design Study Notes (Scaler Academy â€” HLD Module)
> **Instructor:** Tarun Malhotra (Software Engineer at Google)
> **Topics:** Sharding, Replication, CAP Theorem, PACELC Theorem, Master-Slave Architecture, Consistency vs Availability

---

## ðŸ“‹ Table of Contents

### Part 1: Foundations
1. [The Big Picture Architecture](#-the-big-picture-architecture)
2. [Sharding â€” Distributing Data](#-sharding--distributing-data)
3. [Replication â€” Copying Data](#-replication--copying-data)
4. [Sharding vs Replication â€” Side by Side](#-sharding-vs-replication--key-differences)

### Part 2: CAP Theorem
5. [What is CAP Theorem?](#-what-is-cap-theorem)
6. [Consistency (C)](#-c--consistency)
7. [Availability (A)](#-a--availability)
8. [Partition Tolerance (P)](#-p--partition-tolerance)
9. [CAP in Practice â€” The Hardep & Nikl Story](#-cap-in-practice--the-hardep--nikl-story)
10. [Choosing CP vs AP](#-choosing-cp-vs-ap)

### Part 3: PACELC Theorem
11. [What is PACELC Theorem?](#-what-is-pacelc-theorem)
12. [Latency vs Consistency Trade-off](#-latency-vs-consistency-trade-off)
13. [Real-World Examples](#-real-world-examples)

### Part 4: Master-Slave Replication
14. [Master-Slave Architecture](#-master-slave-architecture)
15. [Read Replicas](#-read-replicas)
16. [Failover & Recovery](#-failover--recovery)

### Part 5: Summary & Interview Prep
17. [Quick Reference Cheatsheet](#-quick-reference-cheatsheet)
18. [Practice Exercises & Interview Questions](#-practice-exercises--interview-questions)
19. [Solutions](#-solutions)
20. [References and Resources](#-references-and-resources)

---

# PART 1: FOUNDATIONS

---

## ðŸ—ï¸ The Big Picture Architecture

Before diving into CAP Theorem, let's ground ourselves in how large-scale systems look:

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚         CLIENT (Browser)        â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ DNS Resolution
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚              DNS                â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ IP Address returned
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚       GATEWAY / LOAD BALANCER   â”‚ â—„â”€â”€ SSL termination here
                        â”‚   (First point of contact)      â”‚     Protocol translation
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Rate limiting, etc.
                                 â”‚          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   App Server 1   â”‚                 â”‚   App Server 2   â”‚
         â”‚  (Compute Layer) â”‚                 â”‚  (Compute Layer) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         GLOBAL CACHE             â”‚ â—„â”€â”€ Redis / Memcached
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       STORAGE / DATABASE         â”‚ â—„â”€â”€ Source of Truth
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Key insight:** The Gateway box is called a "gateway" â€” not just a load balancer â€” because it does **much more** than load balancing: SSL termination, protocol translation, rate limiting, authentication, etc.

### Why Multiple App Servers?

| Scenario | Traffic Example |
|----------|----------------|
| Normal Day | Baseline load |
| Black Friday / Big Billion Day | 10-50x spike |
| India vs Pakistan Cricket (Hotstar) | 50-100x spike |
| Diwali Week | Sustained high load |

This is called **elasticity** â€” the ability to **scale up** (add more compute) when traffic is high and **scale down** (remove compute) when traffic is low. Cloud platforms like AWS support this via **Auto Scaling**.

> ðŸ’¡ **Storage â‰  Compute** â€” Compute is highly elastic (scale up/down rapidly). Storage is **deliberately static** â€” you plan ahead, you don't rapidly add/remove DB boxes on the fly.

---

## ðŸ”€ Sharding â€” Distributing Data

### The Problem: Too Much Data for One Machine

Consider Facebook's **User Database**:

```
Users Table (3 Billion Entries):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ user_id â”‚  name  â”‚ gender â”‚ relationship_st  â”‚  DOB     â”‚ last_updated  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   107   â”‚  Raju  â”‚  Male  â”‚    Married       â”‚ 1990-01  â”‚  2024-01-01   â”‚
â”‚   998   â”‚  ...   â”‚  ...   â”‚    ...           â”‚   ...    â”‚     ...       â”‚
â”‚  1070   â”‚  ...   â”‚  ...   â”‚    ...           â”‚   ...    â”‚     ...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Friends Table (Billions more entries):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ user_id â”‚ friend_id â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   107   â”‚    998    â”‚  â† Raju is friends with user 998
â”‚   107   â”‚   1070    â”‚  â† Raju is friends with user 1070
â”‚   107   â”‚    66     â”‚  â† Raju is friends with user 66
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**A single machine CANNOT handle:**
- Storage: ~100 TB+ for billions of users (each with profile, posts, media links...)
- Load: Billions of requests per day

### What is Sharding?

> **Sharding** = Distributing data across multiple machines in a **Mutually Exclusive, Collectively Exhaustive (MECE)** way.

```
UNIVERSE OF ALL DATA
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  user:107  user:998  user:1070  user:66  user:442  user:5001 ... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
               SHARDING splits this into:
                              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼          â–¼          â–¼          â–¼          â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ SHARD A â”‚ â”‚ SHARD B â”‚ â”‚ SHARD C â”‚ â”‚ SHARD D â”‚ â”‚ SHARD E â”‚
  â”‚ u:107   â”‚ â”‚ u:998   â”‚ â”‚ u:1070  â”‚ â”‚ u:66    â”‚ â”‚ u:442   â”‚
  â”‚ u:28    â”‚ â”‚ u:319   â”‚ â”‚ u:2050  â”‚ â”‚ u:7123  â”‚ â”‚ u:9001  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  âœ… All shards TOGETHER = ALL the data (Collectively Exhaustive)
  âœ… No two shards have the SAME data (Mutually Exclusive)
```

### Sharding Keys

The **sharding key** determines which shard a piece of data goes to.

| Sharding Key Type | Example | Used When |
|------------------|---------|-----------|
| **User ID** | `shard = hash(user_id) % num_shards` | User data |
| **Geography** | India â†’ Shard India, US â†’ Shard US | Regional services |
| **Time** | Jan data â†’ Shard 1, Feb data â†’ Shard 2 | Log/event data |
| **Consistent Hash** | Virtual ring with hash function | Distributed caches |

> ðŸ’¡ **Connection to Consistent Hashing**: Remember the circle with servers placed around it? Each server on that circle **is a shard**. The request/user ID is hashed to a position on the ring, and you move clockwise to find the shard.

### Why Keep Related Data in the Same Shard?

This is the concept of **co-location**. Consider this example:

```
Query: "Get all friends of user 107"

âŒ BAD (friends in different shards):
â†’ Query Shard A for 107's friends
â†’ Query Shard B for 107's friends
â†’ Query Shard C for 107's friends
â†’ Aggregate results
= INTERSHARD query (slow, network-heavy, inefficient)

âœ… GOOD (user + their friends in same shard):
â†’ Query Shard A only (107 AND all his friend entries are here)
= INTRASHARD query (fast, single machine, efficient)
```

> **Rule of thumb:** Design your sharding key so that **frequently joined data lives in the same shard**. This converts expensive inter-shard queries into cheap intra-shard queries.

### Intrashard vs Intershard Requests

| Type | Description | Performance | Preferred? |
|------|-------------|-------------|------------|
| **Intrashard** | Query handled within a single shard | Fast âœ… | YES |
| **Intershard** | Query requires going to multiple shards | Slow âŒ | Avoid |

### The Hot Shard Problem ðŸ”¥

A **hot shard** occurs when one shard receives disproportionately more traffic than others.

```
Example: Justin Bieber's data on a celebrity-heavy shard

Shard A (celebrities):  ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥  (millions of read requests)
Shard B (normal users): ðŸ’¡             (low traffic)
Shard C (normal users): ðŸ’¡             (low traffic)
```

**Solutions:**
1. **Hot-specific sharding** â€” Put popular users in dedicated shards with higher replication
2. **Caching** â€” Cache celebrity posts so DB is not hit every time (Facebook News Feed approach)

---

## ðŸ” Replication â€” Copying Data

### The Problem: Too Many Requests for One Machine

```
Scenario: Justin Bieber's posts stored on a single machine

          User 1 â”€â”€â”
          User 2 â”€â”€â”¤
          User 3 â”€â”€â”¤â”€â”€â–º Single DB Machine â—„â”€â”€ ðŸ’¥ Overloaded!
          User 4 â”€â”€â”¤         (Millions of read requests)
          User 5 â”€â”€â”˜
```

Even after sharding, a **single machine per shard** may be overwhelmed by **read requests**.

### What is Replication?

> **Replication** = Creating **multiple identical copies** of the same data on different machines so more users can read from different copies simultaneously.

```
BEFORE Replication:
    Users â”€â”€â–º [Single Copy of Justin Bieber's Posts] â—„â”€â”€ ðŸ’¥ Overloaded

AFTER Replication:
    Users 1-100M  â”€â”€â–º [Copy A â€” Original]       â—„â”€â”€ âœ… Manageable!
    Users 100M-200M â–º [Copy B â€” Replica 1]      â—„â”€â”€ âœ… Manageable!
    Users 200M-300M â–º [Copy C â€” Replica 2]      â—„â”€â”€ âœ… Manageable!
```

### Sharding + Replication Together

These two concepts can and **should** exist simultaneously:

```
3 Billion Facebook Users
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              SHARDING                    â”‚
    â”‚  (Because storage is too large)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚             â”‚              â”‚
           â–¼             â–¼              â–¼
      [SHARD 1]     [SHARD 2]     [SHARD 3]
      Users:1-1B    Users:1B-2B   Users:2B-3B
           â”‚             â”‚              â”‚
           â–¼             â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            REPLICATION                   â”‚
    â”‚  (Because request volume is too high)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚             â”‚              â”‚
     â”Œâ”€â”€â”€â”€â”€â”¤       â”Œâ”€â”€â”€â”€â”€â”¤        â”Œâ”€â”€â”€â”€â”¤
     â”‚     â”‚       â”‚     â”‚        â”‚    â”‚
  [S1 R1][S1 R2] [S2 R1][S2 R2] [S3 R1][S3 R2]
  Shard1  Shard1  Shard2 Shard2  Shard3 Shard3
  Replica1 Repl2  Repl1  Repl2   Repl1  Repl2
```

> ðŸ’¡ Think of each **shard** as a subset of your data universe, and each **replica** as a clone of that shard.

---

## âš–ï¸ Sharding vs Replication â€” Key Differences

| Property | Sharding | Replication |
|----------|----------|-------------|
| **What it does** | Divides data across machines | Copies data to multiple machines |
| **Data on each machine** | DIFFERENT subsets | SAME data |
| **Primary Goal** | Handle too much **data (storage)** | Handle too many **requests (load)** |
| **Mutually Exclusive?** | YES â€” no two shards overlap | NO â€” all replicas are identical |
| **Driven by** | Storage requirements | Request volume / traffic |
| **Analogy** | Splitting a book into chapters | Printing multiple copies of the same book |
| **Scale dimension** | Horizontal **data** scale | Horizontal **read** scale |

```
SHARDING:                           REPLICATION:
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ A-F â”‚ â”‚ G-M â”‚ â”‚ N-Z â”‚           â”‚ ALL â”‚ â”‚ ALL â”‚ â”‚ ALL â”‚
â”‚data â”‚ â”‚data â”‚ â”‚data â”‚           â”‚data â”‚ â”‚data â”‚ â”‚data â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
Different data on each             Same data on each
```

---

# PART 2: CAP THEOREM

---

## ðŸ§© What is CAP Theorem?

> **CAP Theorem** states that in any **distributed system**, you can only guarantee **two out of these three properties simultaneously**:
>
> - **C** â€” Consistency
> - **A** â€” Availability
> - **P** â€” Partition Tolerance

```
              CONSISTENCY (C)
                   â–³
                  / \
                 /   \
                /  âš ï¸  \
               /  Pick  \
              /   2 of 3 \
             /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
AVAILABILITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PARTITION
    (A)                  TOLERANCE (P)
```

| Choice | Properties Guaranteed | Sacrificed |
|--------|----------------------|------------|
| **CA** | Consistent + Available | Partition Tolerance |
| **CP** | Consistent + Partition Tolerant | Availability |
| **AP** | Available + Partition Tolerant | Consistency |

> âš ï¸ **Critical Insight:** In practice, **network partitions ALWAYS happen** (networks are unreliable). Therefore, partition tolerance is **non-negotiable** in distributed systems. The real choice is always between **CP or AP**.

---

## âœ… C â€” Consistency

> **Consistency** means every read returns the **latest write**, OR equivalently, every machine has the **same latest view of the truth**.

```
CONSISTENT System:
                   WRITE: "Raju's DOB = 1990"
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚  Machine 1  â”‚ â”€â”€ "Raju DOB = 1990" âœ…
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Machine 2  â”‚ â”€â”€ "Raju DOB = 1990" âœ…
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

READ from any machine â†’ same answer âœ…

INCONSISTENT System:
                   WRITE went to Machine 1 only:
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Machine 1  â”‚ â”€â”€ "Raju DOB = 1990" âœ…
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Machine 2  â”‚ â”€â”€ "Raju DOB = ???"  âŒ (stale)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

READ from Machine 2 â†’ wrong/missing answer âŒ
```

### Formal Definition

```
Every READ must return the result of the most recent WRITE
(or return an error â€” never stale data)
```

### Real-World Analogy

- ðŸ¦ **Bank ATM**: You deposited â‚¹10 lakh. Your ATM must show â‚¹10 lakh when you check balance â€” not show â‚¹500. Consistency is **non-negotiable** here.
- ðŸ’¹ **Zerodha (Stock Price)**: If Adani stock is â‚¹1200, showing â‚¹1800 to someone else is unacceptable. Wrong price = wrong trade decisions.

---

## âœ… A â€” Availability

> **Availability** means the system **always responds** to every request â€” even if the response isn't the most current data. A system is available if it responds **without error**.

> âš ï¸ Availability does NOT guarantee the response is **correct** â€” just that there IS a response.

```
AVAILABLE System (even if stale):
    User asks: "How many views does this YouTube video have?"
    
    Machine 1 says: "10,242 views"   â† May not be exactly right
    Machine 2 says: "10,238 views"   â† May not be exactly right
    
    But BOTH respond! No error. = AVAILABLE âœ…
    
NOT AVAILABLE System:
    User asks: "What is my ATM balance?"
    
    System says: "Sorry, try again later" â† ERROR response = NOT AVAILABLE âŒ
```

### Real-World Analogy

- ðŸ“º **YouTube view count**: Does not show exact real-time count, but ALWAYS shows *something*. Prioritizes availability.
- ðŸ“º **Hotstar live viewers**: "2.4 million watching" might not be exact, but the count is always there.
- ðŸ¦ **ATM**: Better to say "temporarily unavailable" than show wrong balance. Prioritizes consistency over availability.

---

## âœ… P â€” Partition Tolerance

> **Partition** = A **network partition** â€” a scenario where two machines **temporarily cannot communicate** with each other (the network link between them is broken).

```
NETWORK PARTITION:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         âœ‚ï¸ BROKEN âœ‚ï¸      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Machine 1  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Machine 2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- M1 can still talk to clients âœ…
- M2 can still talk to clients âœ…
- M1 and M2 CANNOT talk to each other âŒ
```

> **Partition Tolerance** = The system **continues to function** even when a network partition occurs.

### Why Partitions Always Happen

Networks are **always unreliable**. Even inside a single data center:
- Cables can fail
- Network ports can drop
- Packets can be lost
- Switches can fail
- Software bugs can block communication

> **Network is always unreliable.** This is a fundamental truth of distributed computing.

---

## ðŸ“– CAP in Practice â€” The Hardep & Nikl Story

Let's walk through a concrete example to understand CP vs AP.

### The Scenario

**Hardep** runs an "Event Reminder Service" â€” people call in to register events; they call back later to recall them. As the service grows, Hardep hires **Nikl**. Now two people serve calls via a **load balancer**.

```
              CLIENTS
             /       \
            /         \
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚     LOAD BALANCER      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”     â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚ HARDEP â”‚     â”‚  NIKL â”‚
â”‚(Diary)â”‚     â”‚(Diary)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Option 1 â€” AP System (Available, Partition Tolerant â€” NOT Consistent)

**Protocol**: Each person writes only in their own diary.

```
WRITE:
  Shiv calls in â†’ Load balancer routes to Hardep
  Hardep writes: "Shiv: girlfriend's wedding, 28 Feb 7pm"
  âœ… Responds immediately

READ (later):
  Shiv calls back â†’ Load balancer routes to Nikl (randomly!)
  Nikl checks his diary: "I have nothing for Shiv"
  âŒ Returns wrong answer â€” "No events found!"
```

**Result:**
- âœ… **Available** â€” Both Hardep and Nikl always respond (no errors)
- âŒ **Not Consistent** â€” Nikl returns wrong data
- âœ… **Partition Tolerant** â€” Even if they stop talking to each other, everything still "works"

---

### Option 2 â€” CP System (Consistent, Partition Tolerant â€” NOT Available)

**Protocol (updated)**: Before acknowledging any write, the receiver writes in BOTH diaries.

```
WRITE:
  Shiv calls in â†’ routes to Hardep
  Hardep writes in his own diary âœ“
  Hardep then calls Nikl: "Please write this too"
  Nikl writes âœ“ and confirms
  ONLY THEN: Hardep tells Shiv "Your event is registered" âœ…

READ (later):
  Shiv calls â†’ routes to Nikl
  Nikl checks diary: "Shiv: girlfriend's wedding, 28 Feb 7pm"
  âœ… Returns correct answer
```

**Problem: What if Nikl goes on holiday (machine goes down)?**

```
Nikl is OFFLINE (vacation in Lonavala! ðŸ–ï¸)

  Shiv calls in â†’ routes to Hardep
  Hardep writes in his own diary âœ“
  Hardep calls Nikl: [no answer] âŒ
  
  HARDEP CANNOT ACKNOWLEDGE THE WRITE â†’ 
  System is UNAVAILABLE for writes âŒ
```

**Result:**
- âœ… **Consistent** â€” When working, every read returns latest write
- âŒ **Not Available** â€” If one machine is down, writes fail completely
- âœ… **Partition Tolerant** â€” Even during partition, the system rejects writes (stays consistent)

---

### Option 3 â€” AP System (with eventual sync)

**Protocol**: Write locally when partner is down; sync when they come back.

```
Nikl is OFFLINE:

  Shiv registers event â†’ Hardep writes locally, acknowledges immediately âœ…
  (Shiv is happy â€” quick response)
  
When Nikl comes BACK ONLINE:
  Hardep sends all pending entries to Nikl
  Nikl writes them âœ“ â€” now both are synced
  
BUT: If network partition happens DURING sync:
  â†’ Some entries may not reach Nikl
  â†’ Nikl comes online, gets some requests
  â†’ Nikl returns stale data âŒ
```

**Result:**
- âœ… **Available** â€” Always responds even when partner is down
- âŒ **Not Consistent** â€” During partition window, stale data possible
- âœ… **Partition Tolerant** â€” Works through network outages

---

### Summary: The Three Options

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Option 1: AP  (each writes locally, no sync)                        â”‚
â”‚  âœ… Always responds (Available)                                       â”‚
â”‚  âœ… Handles partitions fine (Partition Tolerant)                     â”‚
â”‚  âŒ Different machines return different data (Not Consistent)         â”‚
â”‚                                                                      â”‚
â”‚  Option 2: CP  (write to both, fail if one is down)                  â”‚
â”‚  âœ… Every read returns latest write (Consistent)                      â”‚
â”‚  âœ… Handles partitions by refusing to accept inconsistency           â”‚
â”‚  âŒ System is unavailable when one machine is down (Not Available)   â”‚
â”‚                                                                      â”‚
â”‚  Option 3: AP with eventual consistency  (write locally, sync later) â”‚
â”‚  âœ… Always responds (Available)                                       â”‚
â”‚  âœ… Handles partitions (Partition Tolerant)                          â”‚
â”‚  âŒ Stale reads possible during/after partition (Not Consistent)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Choosing CP vs AP

> **In distributed systems, Partition Tolerance is always required.** Networks always fail sometimes. The choice is: **CP or AP**.

### Choose CP (Consistency + Partition Tolerance) when:
- **Banking / Financial systems** â€” Wrong balance is catastrophic
- **Stock Trading (Zerodha)** â€” Wrong price = bad trades
- **IRCTC / Flight Booking** â€” Double-booking a seat is unacceptable
- **User Authentication / Blacklisting** â€” If a user is blocked, they must be blocked everywhere

### Choose AP (Availability + Partition Tolerance) when:
- **YouTube view counts** â€” A slightly stale count is fine; going down is not
- **Hotstar live viewer count** â€” Approximate count is OK
- **Social media likes/comments** â€” A stale like count won't hurt anyone
- **Product catalog / Prices (non-critical)** â€” Slight staleness is acceptable
- **DNS** â€” Propagation delay is acceptable; being unavailable is not

### CA (without Partition Tolerance) â€” Only on Single Machines

| System | Why |
|--------|-----|
| Single SQL database | No network between machines |
| In-process cache | No inter-machine communication |
| SQLite | Single-node, no network partition possible |

> **CA systems exist in practice**, but only for specific use cases within a larger distributed system â€” e.g., a single MySQL box handling blacklist lookups (consistent + available, but no need for partition tolerance because it's ONE machine).

---

# PART 3: PACELC THEOREM

---

## âš¡ What is PACELC Theorem?

PACELC extends CAP theorem by adding a second trade-off that applies **even when there is no partition**.

> **PACELC** = **P**artition â†’ **A**vailability or **C**onsistency; **E**lse â†’ **L**atency or **C**onsistency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PACELC THEOREM                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  IF Partition Happens:                                  â”‚
â”‚     â†’ Choose between AVAILABILITY or CONSISTENCY       â”‚
â”‚        (this is the CAP theorem part)                   â”‚
â”‚                                                         â”‚
â”‚  ELSE (Normal operation, no partition):                 â”‚
â”‚     â†’ Choose between LOW LATENCY or CONSISTENCY        â”‚
â”‚        (this is the NEW insight)                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â±ï¸ Latency vs Consistency Trade-off

### What is Latency?

> **Latency** = the time it takes for a request to be completed.
>
> - **Low Latency** = Fast (good âœ…)
> - **High Latency** = Slow (bad âŒ)

### Why Does Consistency Cost Latency?

Going back to the **Hardep & Nikl** example:

```
HIGH CONSISTENCY Protocol (CP):
  Client writes â†’ Hardep writes locally â†’ Hardep WAITS for Nikl to confirm
                                          â†‘
                                   EXTRA TIME = EXTRA LATENCY

  Total Time: local write + network round trip + Nikl's write + confirmation
```

```
LOW LATENCY Protocol (AP):
  Client writes â†’ Hardep writes locally â†’ Hardep immediately responds âœ…

  Total Time: just local write (fast!)
  But: consistency is sacrificed
```

### The Trade-off Visualized

```
                HIGH CONSISTENCY â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ LOW LATENCY

  Banking ATM   Booking Systems   Social Feed    DNS      View Counts
      â”‚               â”‚                â”‚           â”‚           â”‚
      â–¼               â–¼                â–¼           â–¼           â–¼
   Must be         Must be         Eventual    Stale OK    Stale OK
   consistent      consistent      consistency  (TTL)       (approx)
   (slow ok)       (slow ok)                   (fast!)     (fast!)
```

---

## ðŸŒ Real-World Examples

### IRCTC / Flight Booking (CP + High Latency is OK)

```
User clicks "Book Seat 12A" on IRCTC:

â†’ Request goes to system
â†’ System must ensure NO TWO USERS book the same seat
â†’ Uses distributed lock / 2-phase commit
â†’ This takes 2-3 seconds â† HIGH LATENCY

But: âœ… Nobody ever gets double-booked
```

> You've experienced this! That 2-3 second wait on IRCTC after clicking "Book" â€” that's the system ensuring **consistency** at the cost of **latency**.

### YouTube Views (AP + Low Latency)

```
User visits video:
â†’ System returns view count immediately (maybe slightly stale)
â†’ No waiting for all replica counts to sync
â†’ Response in < 50ms â† LOW LATENCY

But: Different users may see slightly different view counts âŒ (Not consistent)
```

### Zerodha Stock Prices (CP even with latency)

```
Trader checks Adani stock price:
â†’ System MUST return latest price
â†’ Worth a small wait to ensure accuracy
â†’ Wrong data = wrong trades = financial loss
```

---

### PACELC Classification of Common Databases

| Database | Partition | Else | Classification |
|----------|-----------|------|----------------|
| **Cassandra** | A | L | PA/EL â€” Highly available, low latency, eventual consistency |
| **DynamoDB** | A | L | PA/EL â€” Always available, tunable consistency |
| **MongoDB** | C | L | PC/EL â€” Consistent reads, can tune for latency |
| **Zookeeper** | C | C | PC/EC â€” Strong consistency always |
| **MySQL (single node)** | N/A | C | E/C â€” No partition concern, strong consistency |
| **HBase** | C | C | PC/EC â€” Very strong consistency |

---

# PART 4: MASTER-SLAVE ARCHITECTURE

---

## ðŸ›ï¸ Master-Slave Architecture

Master-Slave (also called **Primary-Replica**) is the most common replication pattern.

```
                    CLIENTS
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  LOAD BALANCER  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚             â”‚
    READS â”‚        READS â”‚       READS â”‚
         â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ REPLICA â”‚   â”‚ REPLICA â”‚   â”‚ MASTER  â”‚  â—„â”€â”€ WRITES go here ONLY
    â”‚ (Slave) â”‚   â”‚ (Slave) â”‚   â”‚(Primary)â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚             â”‚             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                    REPLICATION
                  (async/sync)
```

### Key Rules

| Operation | Goes To |
|-----------|---------|
| **WRITE** (INSERT, UPDATE, DELETE) | Master ONLY |
| **READ** (SELECT) | Any Replica (or Master) |

### Why Separate Reads from Writes?

In typical web applications:
- **80-90%** of traffic is **reads**
- **10-20%** of traffic is **writes**

By routing reads to replicas, we dramatically reduce master load.

---

## ðŸ“– Read Replicas

### Synchronous vs Asynchronous Replication

```
SYNCHRONOUS REPLICATION (Strong Consistency):
  
  Client â†’ WRITE â†’ Master
                    â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Replica 1 (write confirmed)
                    â”‚                   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Replica 2 (write confirmed)
                         â†‘
                    Only THEN â†’ ACK to Client
  
  âœ… All replicas always in sync
  âŒ Write latency = slowest replica's write time


ASYNCHRONOUS REPLICATION (Eventual Consistency):
  
  Client â†’ WRITE â†’ Master â†’ ACK to Client immediately âœ…
                    â”‚
                    â””â”€â”€(async)â”€â”€â–º Replica 1 (queued write)
                    â””â”€â”€(async)â”€â”€â–º Replica 2 (queued write)
  
  âœ… Low write latency
  âŒ Replicas may lag â€” reads from replica may be stale
```

### Replication Lag

> **Replication Lag** = the time delay between a write to the master and that write appearing on all replicas.

```
Timeline:
  T+0ms  â†’ User writes post
  T+0ms  â†’ Master has the post âœ…
  T+50ms â†’ Replica 1 has the post âœ…
  T+100ms â†’ Replica 2 has the post âœ…

During T+0ms to T+50ms:
  â†’ If user's next READ goes to Replica 1, they DON'T see their own post! ðŸ˜±
  â†’ This is "Read Your Own Write" consistency violation
```

**Solutions to Replication Lag Issues:**

| Problem | Solution |
|---------|----------|
| User can't read their own write | Route same user's reads to master briefly after writes |
| Monotonic read violation | Always route same user to same replica |
| Lag growing too large | Use synchronous replication for critical data |

---

## ðŸ”„ Failover & Recovery

### What Happens When Master Goes Down?

```
BEFORE FAILURE:
  Master â”€â”€â”€â”€(async)â”€â”€â”€â”€â–º Replica 1
         â”€â”€â”€â”€(async)â”€â”€â”€â”€â–º Replica 2

MASTER FAILS:
  Master âœ—               Replica 1 â† Promoted to new Master!
                         Replica 2 â† Now replicates from Replica 1
```

### Failover Challenges

1. **Data Loss Risk**: If async replication, the failed master may have had some writes that never made it to replicas
2. **Split-Brain Problem**: Two nodes think they're both Master â€” can lead to conflicts
3. **Detection Time**: How quickly do you detect the master failed?
4. **Promotion Complexity**: Which replica becomes the new master? (Usually the most up-to-date one)

### Semi-Synchronous Replication

A middle ground:

```
WRITE â†’ Master stores + sends to at least ONE replica
        â†’ Gets ack from that ONE replica
        â†’ Acknowledges to client

âœ… At most one replica's worth of lag survived
âœ… Better performance than full synchronous
âœ… Better durability than full async
```

---

# PART 5: SUMMARY & INTERVIEW PREP

---

## ðŸ“‹ Quick Reference Cheatsheet

### CAP Theorem Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAP THEOREM CHEATSHEET                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  C (Consistency)     â”‚ Every read returns the latest write   â”‚
â”‚  A (Availability)    â”‚ Every request gets a response (no err)â”‚
â”‚  P (Partition Tol.)  â”‚ System works despite network failures â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CP System           â”‚ Prioritizes accuracy over uptime      â”‚
â”‚  AP System           â”‚ Prioritizes uptime over accuracy      â”‚
â”‚  CA System           â”‚ Only possible on single machine       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  In Distributed?     â”‚ P is always required! â†’ CP or AP only â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PACELC Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PACELC THEOREM CHEATSHEET                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P (Partition)           â”‚ When partition: A vs C            â”‚
â”‚  E (Else / normal)       â”‚ No partition: L vs C              â”‚
â”‚  A (Availability)        â”‚ Respond even if stale             â”‚
â”‚  C (Consistency)         â”‚ Only respond if data is current   â”‚
â”‚  L (Latency)             â”‚ Respond fast (may not sync first) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PA/EL  (Cassandra)      â”‚ AP + fast, eventual consistency   â”‚
â”‚  PC/EC  (Zookeeper)      â”‚ CP + always consistent, slower    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sharding vs Replication Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚      SHARDING        â”‚     REPLICATION      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What it does   â”‚ Splits data across   â”‚ Copies same data to  â”‚
â”‚                â”‚ multiple machines    â”‚ multiple machines    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data per box   â”‚ DIFFERENT subsets    â”‚ SAME full copy       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Solves         â”‚ Storage too large    â”‚ Too many requests    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MECE?          â”‚ Yes â€” exclusive +    â”‚ No â€” all are same    â”‚
â”‚                â”‚ exhaustive           â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Can coexist?   â”‚ YES â€” a shard can    â”‚ YES â€” a shard can    â”‚
â”‚                â”‚ have replicas        â”‚ also be sharded      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Practice Exercises & Interview Questions

### Conceptual Questions

**Q1.** What is the CAP theorem? Explain with a real-world example.

**Q2.** Why can't a distributed system be simultaneously consistent, available, AND partition tolerant?

**Q3.** A social media platform shows slightly stale "likes" count on posts. Is this CP or AP? Why?

**Q4.** An online banking system must ensure that a user cannot withdraw money they don't have, even if two ATMs are used simultaneously. Is this CP or AP? Why?

**Q5.** Explain the difference between sharding and replication. Can they work together?

**Q6.** What is a "hot shard"? How would you solve it in the context of a social network like Instagram?

**Q7.** What is replication lag? When does it cause problems? How do you mitigate it?

**Q8.** What is the PACELC theorem? How does it extend CAP?

**Q9.** When would you choose synchronous replication vs asynchronous replication?

**Q10.** What is an intrashard query vs an intershard query? Which is preferred and why?

---

### System Design Application Questions

**Q11.** You are designing a chess game system. Should you use CP or AP for game state? Why?

**Q12.** You are designing a "recently viewed products" feature on Amazon. Would you use CP or AP? Why?

**Q13.** Design the sharding strategy for a WhatsApp chat system. What would be your sharding key?

**Q14.** You have a single MySQL master that's getting overwhelmed by read queries. What would you do? Walk through the trade-offs.

**Q15.** Facebook stores Justin Bieber's posts. Millions of users try to read his posts per second. How would you design this system? (Hint: hot shard + replication + caching)

---

## ðŸ“ Solutions

### Q1 â€” CAP Theorem Explanation

CAP Theorem states that a distributed system can only guarantee two of three properties: Consistency (every read returns latest write), Availability (every request gets a response), and Partition Tolerance (system works despite network failures).

**Real-world example â€” IRCTC ticket booking:**
- **Consistency**: Two users cannot book the same seat (strong consistency required)
- **Availability**: The booking system should respond even during peak loads
- **Partition Tolerance**: Network issues between servers will inevitably happen

IRCTC chooses **CP**: It's better for the system to be temporarily slow or unavailable than to double-book a seat.

---

### Q2 â€” Why No CAP Together?

When a network partition occurs (which is inevitable), a node must decide: do I respond to the client (Availability) or do I wait until the other node confirms (Consistency)?

- If it responds immediately â†’ Might return stale data â†’ Not Consistent
- If it waits for confirmation â†’ Might timeout â†’ Not Available

There's no way to guarantee both during a partition. Hence: pick one.

---

### Q3 & Q4 â€” AP vs CP Classification

| System | Choice | Reason |
|--------|--------|--------|
| Social media likes count | **AP** | Stale count is fine; availability matters more |
| Bank ATM withdrawal | **CP** | Wrong balance data is catastrophic; prefer unavailability |

---

### Q5 â€” Sharding + Replication Together

Yes! They are complementary, not mutually exclusive.

- **Sharding** handles too much *data* â†’ split across machines
- **Replication** handles too many *requests* â†’ copy to multiple machines

Common pattern: Shard your data into N shards, then replicate each shard M times â†’ N Ã— M total machines.

---

### Q11 â€” Chess Game (CP)

Chess requires **CP** because:
- Both players must see the exact same game state
- An inconsistent view (e.g., player A sees different board than player B) would make the game unplayable
- Latency is acceptable (a few hundred ms is fine per move)

---

### Q12 â€” Recently Viewed Products (AP)

"Recently viewed" is **AP** because:
- Showing a product that was viewed 2 minutes ago instead of 1 minute ago is fine
- The system must ALWAYS respond (user is shopping; showing nothing is worse than showing slightly stale list)
- Consistency is not critical here

---

### Q13 â€” WhatsApp Chat Sharding Key

**Sharding Key: Chat Room ID (or Conversation ID)**

Why:
- All messages in a single chat belong together â†’ intrashard queries for chat history
- Easy to route: hash(chat_id) â†’ shard
- Avoids cross-shard joins for most common queries (fetch messages in a conversation)

Consideration: Group chats with millions of members could become hot shards â†’ may need per-group-specific replication strategy.

---

### Q14 â€” MySQL Overwhelmed by Reads

1. **Add Read Replicas** â€” Route all read queries to replicas
2. **Add Caching Layer** â€” Redis in front of DB for hot queries
3. **Monitor Replication Lag** â€” Ensure replicas don't fall too far behind
4. **Read-Your-Own-Write Routing** â€” Route user's reads to master for N seconds after their write

Trade-off: Adding replicas with async replication â†’ eventual consistency. If strong consistency needed â†’ sync replication (higher latency) or always read from master.

---

### Q15 â€” Justin Bieber's Posts (Hot Shard + Replication + Caching)

Hot account problem (also called "celebrity problem"):

```
Solution layers:
1. Dedicated hot shard for celebrities (separate from normal users)
2. High replication factor for hot shards (e.g., 100 replicas vs 3 for normal users)
3. Aggressive caching â€” cache celebrity posts at CDN / application layer
4. Fan-out on read (pull model) â€” don't push to all followers; pull from celebrity shard when a follower loads feed
```

The Facebook News Feed solution: Use a **Recent Posts DB** (cache) that holds only the latest posts from celebrities, avoiding hitting the main shards on every feed load.

---

## ðŸ“š References and Resources

### Academic & Formal References
- **CAP Theorem** â€” Originally proposed by Eric Brewer (2000), formalized by Gilbert & Lynch (2002)
  - Paper: [Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services](https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf)
- **PACELC Theorem** â€” Proposed by Daniel J. Abadi (2012)
  - Paper: [Consistency Tradeoffs in Modern Distributed Database System Design: CAP is Only Part of the Story](https://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf)

### Database Documentation
- [Cassandra Architecture â€” Data Replication](https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html)
- [MongoDB Replication](https://www.mongodb.com/docs/manual/replication/)
- [MySQL Replication](https://dev.mysql.com/doc/refman/8.0/en/replication.html)
- [PostgreSQL High Availability](https://www.postgresql.org/docs/current/high-availability.html)

### Study Resources
- [Martin Kleppmann â€” Designing Data-Intensive Applications (DDIA)](https://dataintensive.net/) â† **Must Read**
  - Chapter 5: Replication
  - Chapter 6: Partitioning
  - Chapter 9: Consistency and Consensus
- [AWS â€” Eventual Consistency Primer](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html)
- [Cloudflare â€” Why Network Partitions Happen](https://blog.cloudflare.com/a-post-mortem-on-the-recent-cloudflare-outage/)

### Videos & Blogs
- [Martin Fowler â€” CAP Theorem](https://martinfowler.com/articles/distributed-failures.html)
- [ByteByteGo â€” CAP Theorem Simplified](https://blog.bytebytego.com/p/cap-theorem-and-distributed-systems)
- [System Design Primer (GitHub)](https://github.com/donnemartin/system-design-primer#cap-theorem)

---

> ðŸ“Œ **Remember**: CAP Theorem and PACELC are frameworks for *thinking*, not rigid dogmas. Real-world databases often allow you to *tune* consistency vs availability per operation (e.g., DynamoDB's `ConsistentRead`, Cassandra's consistency levels). The key is understanding the **trade-offs** so you can make informed decisions.

