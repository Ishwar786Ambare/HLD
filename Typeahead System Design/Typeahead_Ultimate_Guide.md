# ğŸ” Typeahead System Design â€” The Ultimate HLD Guide

> **Last Updated:** February 2026
> **Author:** System Design Study Notes (Scaler Academy â€” HLD Module)
> **Instructor:** Tarun Malhotra (Software Engineer at Google)
> **Topics:** Typeahead, Autocomplete, Trie, Hashmap, Prefix Search, Sharding, Caching, Debouncing, Sampling, Time Decay

---

## ğŸ“‹ Table of Contents

### Part 1: Introduction & Context
1. [What is Typeahead?](#-what-is-typeahead)
2. [Real-World Examples](#-real-world-examples)
3. [How System Design Interviews Work](#-how-system-design-interviews-work)

### Part 2: Requirements Gathering (MVP)
4. [Functional Requirements](#-functional-requirements)
5. [Non-Functional Requirements / Design Goals](#-non-functional-requirements--design-goals)
6. [Out of Scope (for MVP)](#-out-of-scope-for-mvp)

### Part 3: Capacity Estimation
7. [Traffic Estimation](#-traffic-estimation)
8. [Data Size Estimation](#-data-size-estimation)
9. [Read vs Write Analysis](#-read-vs-write-analysis)

### Part 4: Design Goals
10. [Availability vs Consistency](#-availability-vs-consistency)
11. [Latency Requirements](#-latency-requirements)

### Part 5: API Design
12. [Read API (Typeahead Suggestions)](#-read-api-typeahead-suggestions)
13. [Write API (Search Event)](#-write-api-search-event)

### Part 6: High-Level Architecture
14. [Overall System Architecture](#-overall-system-architecture)
15. [Write Path â€” Async Pipeline](#-write-path--async-pipeline)
16. [Read Path â€” Prefix Lookup](#-read-path--prefix-lookup)

### Part 7: Data Structures
17. [Option A â€” Trie Approach](#-option-a--trie-approach)
18. [Option B â€” Hashmap Approach (Preferred)](#-option-b--hashmap-approach-preferred)
19. [Trie vs Hashmap Comparison](#-trie-vs-hashmap-comparison)

### Part 8: Optimizations
20. [Sharding Strategy](#-sharding-strategy)
21. [Sampling to Reduce Write Load](#-sampling-to-reduce-write-load)
22. [Batching Prefix Updates](#-batching-prefix-updates)
23. [Caching Layer](#-caching-layer)
24. [Time Decay â€” Handling Recency](#-time-decay--handling-recency)
25. [Debouncing on the Client](#-debouncing-on-the-client)

### Part 9: Summary & Interview Prep
26. [Quick Reference Cheatsheet](#-quick-reference-cheatsheet)
27. [Practice Questions & Solutions](#-practice-questions--solutions)
28. [References & Resources](#-references--resources)

---

# PART 1: INTRODUCTION & CONTEXT

---

## ğŸ” What is Typeahead?

> **Typeahead** (also called **Autocomplete** or **Search Suggestion**) is a feature where the system predicts and displays possible completions for the user's partial input **in real-time as they type**.

Every time you type a character into a search bar, the system fires a request and returns a ranked list of suggestions â€” before you have even finished typing.

```
User types: "diw"
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ” diw                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ” diwali                          â”‚
â”‚  ğŸ” diwali 2024 date                â”‚
â”‚  ğŸ” diwali rangoli designs          â”‚
â”‚  ğŸ” diwali wishes in hindi          â”‚
â”‚  ğŸ” diwali crackers                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The core goal is to **reduce the number of characters a user needs to type** to reach their desired search query â€” improving speed, accuracy, and user experience.

### Why Typeahead Matters at Scale

| Platform | Typeahead Volume |
|----------|-----------------|
| Google Search | ~10 billion searches/day â†’ 70 billion typeahead queries/day |
| Amazon | Millions of product searches |
| YouTube | Every search bar interaction |
| URL bar (Chrome) | Every URL you type |

> ğŸ’¡ Each character typed generates a **separate API request** to the typeahead service. A 10-letter search = ~7 typeahead API calls (system activates after 3 characters).

---

## ğŸŒ Real-World Examples

### Google Search Typeahead

```
User starts typing "ipl"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Char 1: "i"         â†’ No suggestions (< 3 chars)
Char 2: "ip"        â†’ No suggestions (< 3 chars)
Char 3: "ipl"       â†’ Suggestions appear:
                        â€¢ ipl 2024
                        â€¢ ipl points table
                        â€¢ ipl teams
                        â€¢ ipl schedule
Char 4: "ipl "      â†’ Refined suggestions
Char 5: "ipl 2"     â†’ More refined
... and so on
```

### Amazon Product Typeahead

```
User types: "airp"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ airpods                    â† product name
â€¢ airpods pro                â† product variant
â€¢ airpods max                â† product variant
â€¢ airpods case               â† accessory
â€¢ airpods 3rd generation     â† specific generation
```

> âš ï¸ **Important Distinction:** Typeahead â‰  Search Results.
> - **Typeahead** shows *query suggestions* (what to search for)
> - **Search Results** show *actual results* (products, links, pages)
> They are **two completely separate systems**.

### Difference: Google vs Amazon Typeahead

| Platform | Typeahead Shows | Example |
|----------|----------------|---------|
| **Google** | Related search queries | "diwali recipes", "diwali 2024" |
| **Amazon** | Product names & categories | "airpods pro", "airpods case" |
| **YouTube** | Video-related queries | "ipl 2024 highlights" |
| **URL bar** | Previously visited URLs | "https://github.com/..." |

---

## ğŸ¯ How System Design Interviews Work

Before jumping into the design, it is critical to understand **how HLD interviews are structured**. The instructor emphasized this strongly.

### The Structured HLD Interview Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HLD INTERVIEW FRAMEWORK                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 1: Clarify the Problem                                 â”‚
â”‚    â†’ Ask clarifying questions to narrow scope                â”‚
â”‚    â†’ Understand the product deeply                           â”‚
â”‚                                                              â”‚
â”‚  STEP 2: Define MVP (Functional Requirements)                â”‚
â”‚    â†’ What features MUST the system support?                  â”‚
â”‚    â†’ Keep scope reasonable for a 45-min interview            â”‚
â”‚                                                              â”‚
â”‚  STEP 3: Estimate Scale                                      â”‚
â”‚    â†’ Users, Queries Per Second, Data Size                    â”‚
â”‚    â†’ Helps decide: sharding? caching? replication?           â”‚
â”‚                                                              â”‚
â”‚  STEP 4: Define Design Goals (Non-Functional)                â”‚
â”‚    â†’ Latency? Consistency? Availability?                     â”‚
â”‚    â†’ These guide architectural decisions                     â”‚
â”‚                                                              â”‚
â”‚  STEP 5: API Design                                          â”‚
â”‚    â†’ What endpoints does the system expose?                  â”‚
â”‚                                                              â”‚
â”‚  STEP 6: High-Level Design                                   â”‚
â”‚    â†’ Draw the architecture                                   â”‚
â”‚    â†’ Explain data flow (read path, write path)               â”‚
â”‚                                                              â”‚
â”‚  STEP 7: Deep Dive & Optimizations                           â”‚
â”‚    â†’ Drill into critical components                          â”‚
â”‚    â†’ Discuss trade-offs                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> âš ï¸ **Key interview insight:** Your job as a **system architect** is to **execute and build** the vision â€” not to ideate features. If you start suggesting new features the interviewer didn't ask for, you signal poor focus. Ask product questions to clarify scope, then build it.

---

# PART 2: REQUIREMENTS GATHERING (MVP)

---

## âœ… Functional Requirements

These are the **user-facing features** of the MVP. They describe what the system *does*, not *how* it does it.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               TYPEAHEAD MVP â€” FUNCTIONAL REQUIREMENTS         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  FR1: Show top suggestions as user types                     â”‚
â”‚       â†’ Suggestions appear after the user types â‰¥ 3 chars   â”‚
â”‚       â†’ Return top 5 suggestions                            â”‚
â”‚                                                              â”‚
â”‚  FR2: Suggestions ranked by global popularity                â”‚
â”‚       â†’ Most frequently searched queries appear first        â”‚
â”‚       â†’ No personalization for MVP                           â”‚
â”‚                                                              â”‚
â”‚  FR3: Region-based suggestions                               â”‚
â”‚       â†’ Suggest queries popular in the user's region         â”‚
â”‚       â†’ E.g., "diwali" trends differently in India vs US     â”‚
â”‚                                                              â”‚
â”‚  FR4: No results = No suggestions                            â”‚
â”‚       â†’ If prefix has no popular completions, show nothing   â”‚
â”‚       â†’ Don't show irrelevant suggestions                    â”‚
â”‚                                                              â”‚
â”‚  FR5: Handle misspellings gracefully                         â”‚
â”‚       â†’ Long or garbled queries may return zero results      â”‚
â”‚       â†’ This is acceptable for MVP                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Clarifying Questions to Ask in Interview

These are the questions you should **proactively ask** the interviewer:

| Question | Why it Matters |
|----------|---------------|
| How many suggestions to show? | Affects storage (top N per prefix) |
| After how many characters do we show suggestions? | Affects query volume |
| Do we need personalization? | Massive complexity if yes |
| Multi-language support? | Affects character encoding and data size |
| Real-time ranking updates or batch? | Affects write architecture |
| Region-specific or global? | Affects data partitioning |

### MVP Decision Table

| Feature | In MVP? | Reason |
|---------|---------|--------|
| Global popularity ranking | âœ… YES | Core feature |
| Show top 5 suggestions | âœ… YES | Standard UX |
| Min 3 characters to trigger | âœ… YES | Reduces noise |
| Region-based suggestions | âœ… YES | Important for relevance |
| Personalization by user | âŒ NO | Too complex for MVP |
| Spell correction | âŒ NO | Separate ML system |
| Real-time trending (sub-second) | âŒ NO | Eventual consistency is fine |
| Multi-language (Unicode) | âœ… YES | Global system |

---

## ğŸ¯ Non-Functional Requirements / Design Goals

Design goals describe **how the system behaves**, not what it does. They guide architectural decisions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TYPEAHEAD â€” DESIGN GOALS (Non-Functional)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  DG1: LATENCY â€” Ultra-low read latency                       â”‚
â”‚       â†’ Suggestions must appear within 10ms of each keypress â”‚
â”‚       â†’ User types faster than suggestions should lag        â”‚
â”‚                                                              â”‚
â”‚  DG2: AVAILABILITY â€” Highly available                        â”‚
â”‚       â†’ System must always return suggestions                â”‚
â”‚       â†’ Prefer availability over strict consistency          â”‚
â”‚                                                              â”‚
â”‚  DG3: EVENTUAL CONSISTENCY â€” Stale data acceptable           â”‚
â”‚       â†’ Ranking updates can lag by minutes/hours             â”‚
â”‚       â†’ A slightly stale top-5 list is perfectly fine        â”‚
â”‚                                                              â”‚
â”‚  DG4: SCALABILITY â€” Handle 10B+ queries/day                  â”‚
â”‚       â†’ Peak load of 1M+ typeahead requests/second           â”‚
â”‚       â†’ Must scale horizontally                              â”‚
â”‚                                                              â”‚
â”‚  DG5: FAULT TOLERANCE                                        â”‚
â”‚       â†’ Single machine failure must not bring down typeahead â”‚
â”‚       â†’ Replicate data across availability zones             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **Key Design Goal Insight from Lecture:**
> Latency is the *most critical* design goal for typeahead.
> The suggestions must appear *faster than the user can type* â€” ideally under **10 milliseconds** for the server-side response.
> If suggestions lag behind typing speed, users stop looking at them entirely.

---

## âŒ Out of Scope (for MVP)

| Feature | Why Out of Scope |
|---------|-----------------|
| Spell correction / fuzzy matching | Separate ML pipeline |
| Personalized suggestions per user | Requires user history & complex ranking |
| Search result pages | Different system entirely |
| Voice search | Different input modality |
| Trending / real-time viral queries | Can be added post-MVP |
| Image/video suggestions in dropdown | Rich media = separate feature |

---

# PART 3: CAPACITY ESTIMATION

---

## ğŸ“Š Traffic Estimation

### Step 1: Establish Baseline

```
Given:
  Daily Active Users (DAU) = 500 million (Google scale)
  Searches per user per day = ~20
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total searches per day = 500M Ã— 20 = 10 Billion searches/day
```

### Step 2: Typeahead Multiplier

Every search query generates multiple typeahead requests:

```
A typical search query = ~10 characters
Typeahead activates after 3rd character
â†’ Characters that trigger typeahead = 10 - 3 = 7 characters
â†’ So: 1 search = 7 typeahead API calls

Total typeahead requests/day = 10B Ã— 7 = 70 Billion/day
```

### Step 3: Convert to Queries Per Second (QPS)

```
Seconds in a day = 24 Ã— 60 Ã— 60 = 86,400 seconds

Average typeahead QPS = 70,000,000,000 / 86,400 â‰ˆ 800,000 QPS

Peak QPS (assume 2-3x average) â‰ˆ 2,000,000 QPS (2M/sec)
```

```
TRAFFIC SUMMARY:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Metric                    â”‚         Value               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Daily searches            â”‚  10 Billion/day             â”‚
â”‚  Typeahead multiplier      â”‚  7x per search              â”‚
â”‚  Total typeahead requests  â”‚  70 Billion/day             â”‚
â”‚  Average Read QPS          â”‚  ~800,000 QPS               â”‚
â”‚  Peak Read QPS             â”‚  ~2,000,000 QPS             â”‚
â”‚  Write QPS (search events) â”‚  ~100,000 QPS               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **Why write QPS is lower**: Writes happen only on full search submissions, not on every keypress. So 10B searches/day Ã· 86,400 = ~100K writes/second.

---

## ğŸ’¾ Data Size Estimation

### What Data Do We Store?

For each unique search query, we store:

```
Row = { query_string, hit_count }

- query_string: avg 30-32 characters (includes multi-language, spaces, Unicode)
- hit_count:    8 bytes (to hold billions of hits â€” we need more than 4 bytes)
- overhead:     ~60 bytes (metadata, indices, etc.)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total per row â‰ˆ 100 bytes
```

### How Many Unique Queries?

```
Total daily searches       = 10 Billion
Unique queries (â‰ˆ 15%)     = 10B Ã— 0.15 = 1.5 Billion unique queries/day

Note: ~85% of daily searches are REPEAT queries
(e.g., "weather today", "ipl score" are searched millions of times daily)
```

### Total Storage Required

```
Data per day = 1.5 Billion unique queries Ã— 100 bytes
             = 150 Billion bytes
             = 150 GB/day of NEW unique queries

BUT: Most queries persist across days (they accumulate)
Accumulation over 1 year â‰ˆ 150 GB Ã— 365 = ~55 TB
(manageable with sharding)
```

### Prefix Storage (for Hashmap approach)

Each stored query also generates prefix entries:

```
Query "diwali" (6 chars) â†’ prefixes: "diw", "diwa", "diwal", "diwali"
Average query (10 chars) â†’ ~7 prefix entries

Prefix storage multiplier â‰ˆ 7x
Total prefix data â‰ˆ 150 GB Ã— 7 = ~1 TB/day of prefix entries
```

> âš ï¸ **This is why sharding is essential** â€” 1 TB/day of prefix data cannot be managed by a single machine.

---

## ğŸ“ˆ Read vs Write Analysis

```
SYSTEM CHARACTERIZATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  READ QPS  : ~800,000 - 2,000,000 per second  â† READ HEAVY
  WRITE QPS : ~100,000 per second               â† Write moderate

  READ : WRITE ratio = ~8:1 to 20:1

  Verdict: This is a READ-HEAVY, I/O-BOUND system
           â†’ Optimize heavily for read performance
           â†’ Use caching aggressively
           â†’ Accept eventual consistency to reduce write complexity
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

# PART 4: DESIGN GOALS

---

## âš–ï¸ Availability vs Consistency

Applying CAP Theorem to the typeahead problem:

```
Question: If two users search from different regions simultaneously,
is it acceptable that one user sees "diwali 2024" in top 5
while another user sees "diwali greetings" in top 5?

Answer: YES â€” absolutely acceptable!
```

**Reasoning:**

| Property | Decision | Reason |
|----------|----------|--------|
| **Availability** | âœ… PRIORITIZE | System must ALWAYS return suggestions. Going down = terrible UX |
| **Consistency** | âŒ RELAX | Stale top-5 list (off by 1-2%) is completely invisible to users |
| **Partition Tolerance** | âœ… REQUIRED | Distributed system â€” partitions will happen |

> **Typeahead is an AP system** â€” Available + Partition Tolerant.
> We accept **eventual consistency**: rankings may lag by minutes or even hours, and that is perfectly acceptable.

### Why Consistency Doesn't Matter Much Here

```
If "diwali" has been searched:
  Actual count : 10,000,000 times
  Our cached count: 9,985,000 times (0.15% off)

Does this change the top 5 suggestions? Almost never.
Does the user notice? Never.

Conclusion: Minor inconsistency in hit counts = zero user impact.
```

---

## âš¡ Latency Requirements

```
Typeahead latency budget:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User types a key                            â”‚
â”‚         â”‚                                    â”‚
â”‚         â–¼ (debounce: wait for pause)         â”‚
â”‚  Client sends API request                    â”‚
â”‚         â”‚                                    â”‚
â”‚         â–¼                                    â”‚
â”‚  Network transit to server    â‰ˆ 5-10ms       â”‚
â”‚         â”‚                                    â”‚
â”‚         â–¼                                    â”‚
â”‚  Server processes + DB lookup â‰ˆ 1-5ms        â”‚  â† Our target
â”‚         â”‚                                    â”‚
â”‚         â–¼                                    â”‚
â”‚  Response travels back        â‰ˆ 5-10ms       â”‚
â”‚         â”‚                                    â”‚
â”‚         â–¼                                    â”‚
â”‚  Browser renders suggestions  â‰ˆ 1-2ms        â”‚
â”‚                                              â”‚
â”‚  TOTAL: ~15-30ms end-to-end                  â”‚
â”‚         SERVER SIDE TARGET: < 10ms           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ¯ **Server-side target: < 10ms per typeahead lookup**
> This means we CANNOT do complex DB queries. We need a **cache-first** read path.

---

# PART 5: API DESIGN

---

## ğŸ“¡ Read API (Typeahead Suggestions)

```
GET /typeahead/suggestions

Query Parameters:
  prefix   (string, required)  â€” The partial query typed by user (min 3 chars)
  region   (string, optional)  â€” User's region code (e.g., "IN", "US")
  limit    (integer, optional) â€” Number of suggestions (default: 5)

Response:
{
  "prefix": "diw",
  "suggestions": [
    "diwali",
    "diwali 2024 date",
    "diwali rangoli",
    "diwali wishes",
    "diwali crackers"
  ],
  "region": "IN"
}
```

**Protocol choice: HTTP/REST with long-polling or WebSocket**

| Protocol | Latency | Notes |
|----------|---------|-------|
| REST/HTTP | âœ… Good | Simple, cacheable, widely supported |
| WebSocket | âœ… Better | Persistent connection, no handshake overhead per key |
| gRPC | âœ… Best (internal) | Use for service-to-service communication |

> For users â†’ REST is fine. Between internal services â†’ gRPC preferred.

---

## ğŸ“ Write API (Search Event)

When a user **submits** a search (hits Enter or clicks a result), we record this event:

```
POST /search/event

Request Body:
{
  "query": "diwali rangoli designs",
  "user_id": "u12345",      (for logging â€” not used in ranking for MVP)
  "region": "IN",
  "timestamp": 1708700000
}

Response:
{
  "status": "accepted",
  "message": "Search event queued for processing"
}
```

> âš ï¸ **This API is asynchronous!** The response is immediate ("accepted"), but the actual update to hit counts happens via an async pipeline. The user does not wait for the ranking to update.

---

# PART 6: HIGH-LEVEL ARCHITECTURE

---

## ğŸ—ï¸ Overall System Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                  CLIENTS                     â”‚
                    â”‚           (Browser / Mobile App)             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚     GATEWAY / CDN /        â”‚
                          â”‚     LOAD BALANCER          â”‚
                          â”‚  (SSL, Rate Limiting,      â”‚
                          â”‚   Request Routing)         â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  TYPEAHEAD    â”‚    â”‚   SEARCH           â”‚
                    â”‚  SERVICE      â”‚    â”‚   SERVICE          â”‚
                    â”‚  (Read API)   â”‚    â”‚   (Write API)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                  â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   CACHE       â”‚         â”‚   MESSAGE QUEUE     â”‚
               â”‚   (Redis)    â”‚         â”‚   (Kafka)           â”‚
               â”‚  prefixâ†’top5 â”‚         â”‚   (async pipeline)  â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   PREFIX STORE        â”‚    â”‚   CONSUMER SERVICE   â”‚
           â”‚   (Key-Value DB)      â”‚    â”‚   (updates counts,   â”‚
           â”‚   prefix â†’ top5 list  â”‚    â”‚    recalculates top5)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   HIT COUNT STORE        â”‚
                                   â”‚   (Key-Value DB)         â”‚
                                   â”‚   query â†’ hit_count      â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The system has **two separate paths**:

| Path | Trigger | Goal |
|------|---------|------|
| **Read Path** | User types a character | Return top 5 suggestions in < 10ms |
| **Write Path** | User submits a search | Update hit counts and recalculate rankings (async) |

---

## âœï¸ Write Path â€” Async Pipeline

```
Step 1: User submits search "diwali rangoli designs"
         â”‚
         â–¼
Step 2: Search Service receives event
         â”‚
         â–¼
Step 3: Search Service publishes message to Kafka topic: "search-events"
         {query: "diwali rangoli designs", region: "IN", timestamp: ...}
         â”‚
         â–¼
Step 4: Consumer reads from Kafka (batch or per-message)
         â”‚
         â”œâ”€â–º Upsert HIT COUNT STORE: "diwali rangoli designs" â†’ count+1
         â”‚
         â””â”€â–º Every N updates (sampling - see Section 8.2):
              Update PREFIX STORE for all prefixes of this query:
              "diw"   â†’ recalculate top 5
              "diwa"  â†’ recalculate top 5
              "diwal" â†’ recalculate top 5
              ... etc.
```

### Why Kafka/Async?

Without async processing, every search hit would require:
1. Updating the hit count (1 write)
2. Updating all prefix entries (~7 writes)
3. Total: ~8 synchronous writes per search

At 100,000 searches/sec â†’ **800,000 synchronous writes/second** â€” unsustainable!

With Kafka:
- Search service: just publish to queue â†’ done (< 1ms)
- Consumer: processes messages at its own pace
- Write load is decoupled from read load

---

## ğŸ“– Read Path â€” Prefix Lookup

```
Step 1: User types "diw"

Step 2: Client fires: GET /typeahead/suggestions?prefix=diw&region=IN

Step 3: Typeahead Service checks Redis Cache:
         key = "IN:diw"
         â†’ CACHE HIT? â†’ Return immediately âœ… (< 1ms)
         â†’ CACHE MISS? â†’ Go to Prefix Store

Step 4: Prefix Store lookup:
         key = "diw"
         â†’ Returns: ["diwali", "diwali 2024", "diwali rangoli", ...]

Step 5: Return top 5 to client

Step 6: Populate cache for next time:
         Redis: "IN:diw" â†’ ["diwali", "diwali 2024", ...]  TTL = 5 min
```

```
READ PATH LATENCY BREAKDOWN:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cache Hit Path:   < 1ms (Redis in-memory lookup)       â”‚
â”‚  Cache Miss Path:  3-5ms (goes to Prefix Store)         â”‚
â”‚  Target:           < 10ms total server-side             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ¯ **Cache hit rate for typeahead is very high** â€” popular prefixes like "diw", "ipl", "wea" are searched millions of times per minute. Once cached, the same prefix is served from Redis for TTL duration.

---

# PART 7: DATA STRUCTURES

---

## ğŸŒ³ Option A â€” Trie Approach

A **Trie** (also called a **Prefix Tree**) is a tree data structure where each node represents one character, and paths from root to nodes represent prefixes.

```
Inserted queries: "diwali", "diwa", "dawn", "day"

TRIE STRUCTURE:

         (root)
           â”‚
           d
           â”‚
       â”Œâ”€â”€â”€â”´â”€â”€â”€â”
       i       a
       â”‚       â”‚
       w    â”Œâ”€â”€â”´â”€â”€â”
       â”‚    w     y
    â”Œâ”€â”€â”´â”€â”€â” â”‚     â”‚
    a     (end) n  (end)
    â”‚           â”‚
 â”Œâ”€â”€â”´â”€â”€â”      (end)
 l    (end)
 â”‚
 i
 â”‚
(end) â† "diwali" complete
```

### Trie Node Structure

```python
class TrieNode:
    character: str           # the character this node represents
    is_terminal: bool        # True if this node ends a complete query
    hit_count: int           # frequency of this query (if terminal)
    top5: List[str]          # top 5 suggestions reachable from this node
    children: Dict[str, TrieNode]   # child nodes (one per next character)
```

### How Reads Work in a Trie

```
Read Query: prefix = "diw"

Step 1: Start at root
Step 2: Navigate â†’ 'd' â†’ 'i' â†’ 'w'
Step 3: Return the top5 stored at the "diw" node
        â†’ ["diwali", "diwali 2024", "diwali rangoli", ...]

Time Complexity: O(len(prefix)) â€” very fast!
```

### How Writes Work in a Trie

```
Write: "diwali" gets +1 hit

Step 1: Navigate to "diwali" terminal node
Step 2: Increment hit_count at "diwali" node
Step 3: Walk BACK UP the tree, updating top5 at each ancestor:
         â†’ Update "diwal" node's top5
         â†’ Update "diwa" node's top5
         â†’ Update "diw" node's top5
         â†’ Update "di" node's top5
         â†’ Update "d" node's top5
         â†’ Update root's top5

Time Complexity: O(len(query)) for traversal + O(N log 5) for top5 recalc
```

### Trie Problems at Scale

```
âŒ Problem 1: Memory â€” A full trie for all Google queries cannot fit in RAM
              â†’ Must be stored on disk â†’ slow lookups

âŒ Problem 2: Single-machine trie â†’ hot spot, no distribution

âŒ Problem 3: Write amplification â€” one query update touches ALL ancestor nodes
              â†’ For "diwali rangoli designs" (23 chars) â†’ updates 23 nodes
              â†’ At 100K writes/sec â†’ millions of node updates/sec

âŒ Problem 4: Row locking on every write â†’ read concurrency is blocked

âœ… Trie is GREAT for interviews to explain concept
   Use HASHMAP approach for production
```

---

## ğŸ—ºï¸ Option B â€” Hashmap Approach (Preferred)

Instead of a tree structure, use **two separate hashmaps** (implemented as key-value stores):

```
HASHMAP 1: PREFIX STORE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key: prefix string (e.g., "diw")
Value: ordered list of top 5 suggestions

  "d"    â†’ ["divorce", "diwali", "download", "drive", "dubai"]
  "di"   â†’ ["disney", "diwali", "diet", "diamond", "discord"]
  "diw"  â†’ ["diwali", "diwali 2024", "diwali rangoli", "diwali wishes", "diwali songs"]
  "diwa" â†’ ["diwali", "diwali 2024", "diwali rangoli", "diwali puja", "diwali songs"]
  ...


HASHMAP 2: HIT COUNT STORE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key: full search query (e.g., "diwali")
Value: hit count (number of times this exact query was searched)

  "diwali"               â†’ 10,000,000
  "diwali 2024"          â†’ 4,500,000
  "diwali rangoli"       â†’ 2,300,000
  "diwali wishes"        â†’ 1,800,000
  "diwali puja vidhi"    â†’ 900,000
  ...
```

### Read Query â€” How It Works

```
User types "diw":

  1. Typeahead Service: lookup PREFIX STORE["diw"]
  2. PREFIX STORE returns: ["diwali", "diwali 2024", ...]
  3. Return to user immediately

Time: O(1) hash lookup â€” extremely fast!
No tree traversal needed.
```

### Write Query â€” How It Works

```
Search event: query = "diwali"

Step 1: Update HIT COUNT STORE:
         "diwali" â†’ current_count + 1   (upsert)

Step 2: Recalculate top 5 for ALL prefixes of "diwali":
         prefixes = ["d", "di", "diw", "diwa", "diwal", "diwali"]

         For each prefix p:
           â†’ Get current top5[p] from PREFIX STORE
           â†’ Check if "diwali"'s new count bumps into the top 5
           â†’ If yes: update PREFIX STORE[p] with new top 5 list
           â†’ If no: no update needed (optimization!)
```

### Two-Hashmap Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KEY-VALUE STORES                              â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              PREFIX STORE (Read-optimized)                â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚   "diw"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ["diwali", "diwali 2024", ...]  â”‚   â”‚
â”‚  â”‚   "ipl"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ["ipl 2024", "ipl teams", ...]  â”‚   â”‚
â”‚  â”‚   "wea"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ["weather", "weather today",...]â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚   READ: O(1)  |  Used by: Typeahead Service              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           HIT COUNT STORE (Write-optimized)               â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚   "diwali"            â”€â–º 10,000,000                      â”‚   â”‚
â”‚  â”‚   "diwali 2024"       â”€â–º 4,500,000                       â”‚   â”‚
â”‚  â”‚   "ipl 2024 schedule" â”€â–º 7,200,000                       â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚   WRITE: O(1)  |  Used by: Consumer Service              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš–ï¸ Trie vs Hashmap Comparison

| Property | Trie | Hashmap (Key-Value) |
|----------|------|---------------------|
| **Read speed** | O(len prefix) â€” fast | O(1) â€” faster |
| **Write complexity** | Traverse + update all ancestors | Compute prefixes independently |
| **Memory footprint** | High (tree overhead) | Lower (flat keys) |
| **Sharding** | Very hard (tree can't split) | Easy (key-based shard) |
| **Distribution** | Hard to distribute | Native in KV stores |
| **Production use** | âŒ Not ideal at Google scale | âœ… Preferred |
| **Interview explanation** | âœ… Great for conceptual talk | âœ… Final answer |

---

# PART 8: OPTIMIZATIONS

---

## ğŸ”€ Sharding Strategy

### Why We Need Sharding

```
Total data in PREFIX STORE:
  ~1 TB/day of prefix entries
  â†’ Cannot fit on one machine
  â†’ Cannot be served from one machine at 800K QPS

Solution: SHARD by prefix
```

### Sharding by Prefix

```
SHARDING STRATEGY: Prefix-based partitioning

  Shard 1 â”€â–º all prefixes starting with "a" â†’ "e"
  Shard 2 â”€â–º all prefixes starting with "f" â†’ "j"
  Shard 3 â”€â–º all prefixes starting with "k" â†’ "o"
  Shard 4 â”€â–º all prefixes starting with "p" â†’ "t"
  Shard 5 â”€â–º all prefixes starting with "u" â†’ "z"

When user types "diw":
  â†’ Hash("d") â†’ routes to Shard 1
  â†’ Only ONE shard is hit per read request â†’ intrashard query âœ…

When consumer updates "diwali":
  â†’ All prefixes ("d","di","diw",...) go to the SAME shard
  â†’ Because all start with "d" â†’ all in Shard 1
  â†’ Consistent, no cross-shard coordination needed âœ…
```

### Hot Shard Problem

```
âš ï¸ Problem: Letters like "s", "a", "t", "b" are more common
             â†’ Shards for these letters get more traffic

Solution:
  1. Sub-shard hot letters: "s" â†’ "sa"-"sm" on Shard A, "sn"-"sz" on Shard B
  2. Add more replicas for hot shards
  3. Cache aggressively for hot prefixes
```

---

## ğŸ² Sampling to Reduce Write Load

### The Problem

At 100,000 searches/second for "diwali", we'd need:
- 100,000 HIT COUNT updates/second for just "diwali"
- 100,000 Ã— 6 = 600,000 PREFIX STORE updates/second for "diwali" prefixes

This is catastrophically expensive. **Row locking** during updates means reads are blocked!

### The Sampling Solution

> **Key Insight:** We don't need the EXACT count of "diwali" searches. We just need to know it's MORE POPULAR than "diwali 2024". The relative ranking is what matters.

```
WITHOUT SAMPLING:
  Every search for "diwali" â†’ write to DB
  100,000 searches/sec â†’ 100,000 writes/sec

WITH SAMPLING (sample every 1000th):
  Only 1 in 1000 searches triggers a write to DB
  100,000 searches/sec â†’ 100 writes/sec

  The count stored = actual_count / 1000
  But relative rankings remain EXACTLY the same âœ…

Analogy: Exit polls sample 1000 voters to predict a 100M election
         The sample still reveals the winner accurately
```

### Sampling Implementation

```python
def handle_search_event(query: str):
    # Increment local in-memory counter (cheap)
    in_memory_cache[query] += 1

    # Only write to DB every N-th search
    SAMPLE_RATE = 1000
    if in_memory_cache[query] % SAMPLE_RATE == 0:
        # Write to HIT COUNT STORE
        db.upsert(query, in_memory_cache[query])
        # Update all prefix entries in PREFIX STORE
        update_prefix_store(query)
```

---

## ğŸ“¦ Batching Prefix Updates

### The Problem

Even with sampling, updating prefix stores is expensive:

```
Query "diwali rangoli designs" (23 chars) â†’ 20 prefix updates
  "d", "di", "diw", "diwa", ... "diwali rangoli design", "diwali rangoli designs"

But we also use sampling â†’ prefix update happens every 1000th write

So: 1 prefix scan per query type per 1000 events
    = much more manageable
```

### Optimization: Skip Prefix Update if Not in Top 5

```
When updating prefix for "diw" because "diwali" got +1000 hits:

  1. Read current top5["diw"] from PREFIX STORE
  2. Is "diwali" already in top5["diw"]? YES
  3. Is "diwali"'s new count still enough to stay in top 5? YES
  4. No change to PREFIX STORE needed â†’ SKIP THE WRITE âœ…

This dramatically reduces actual writes to the prefix store
(only write when a new entry enters or exits the top 5)
```

### Write Amplification Reduction

```
WITHOUT batching + sampling:
  100K searches/sec Ã— 20 prefixes = 2M prefix writes/sec

WITH sampling (1/1000) + skip-if-unchanged optimization:
  Only ~200 meaningful prefix updates/sec â†’ 10,000x reduction âœ…
```

---

## ğŸ—ƒï¸ Caching Layer

### Redis Cache in Front of Prefix Store

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CACHE STRATEGY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Cache: Redis (in-memory key-value)                        â”‚
â”‚  Key format: "{region}:{prefix}"  (e.g., "IN:diw")        â”‚
â”‚  Value: JSON array of top 5 suggestions                    â”‚
â”‚  TTL: 5-10 minutes (stale is fine for typeahead)           â”‚
â”‚                                                            â”‚
â”‚  Cache Hit  â†’ return immediately, ~0.1ms                   â”‚
â”‚  Cache Miss â†’ fetch from Prefix Store (~3-5ms)             â”‚
â”‚              â†’ populate cache for future requests          â”‚
â”‚                                                            â”‚
â”‚  Cache Hit Rate: ~95%+ for popular prefixes                â”‚
â”‚  (most users type similar prefixes â€” "wea", "ipl"          â”‚
â”‚   are searched millions of times per minute)               â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cache Invalidation

When the PREFIX STORE is updated (a new query enters top 5):
```
  1. Consumer updates PREFIX STORE["diw"] with new top 5
  2. Consumer also deletes/updates Redis cache entry for "diw"
  3. Next request fetches fresh data from PREFIX STORE
  4. Cache repopulated

OR: Let TTL expire naturally (acceptable because slight staleness is fine)
```

---

## â° Time Decay â€” Handling Recency

### The Problem

A query that trended in 2020 (e.g., "covid vaccine") still has millions of historical hits. In 2024, it may no longer be relevant.

```
Pure frequency ranking:
  "covid vaccine"          â†’ 50,000,000 hits (mostly from 2020-2021)
  "covid vaccine 2024"     â†’ 100,000 hits
  
  Top result would show "covid vaccine" even if nobody searches it now.
  This is stale and wrong!
```

### Solution: Time Decay (Decay Factor)

> **Decay**: Periodically reduce all hit counts slightly to allow newer queries to rise naturally.

```
DECAY ALGORITHM (runs periodically, e.g., every hour):

For every query in HIT COUNT STORE:
  new_count = (current_count Ã— 0.99) - fixed_amount

Example:
  "covid vaccine"      : 50,000,000 â†’ 50,000,000 Ã— 0.99 - 1000 = 49,499,000
  "diwali rangoli 2024": 100,000    â†’ 100,000 Ã— 0.99 - 1000 = 98,000
  "some old query"     : 500        â†’ 500 Ã— 0.99 - 1000 = -505 â†’ DELETE âœ…

THRESHOLD: If count drops below 100, remove entry from the store
â†’ This auto-cleans garbage/spam/one-off queries
```

### Why Multiply AND Subtract?

```
Multiply by 0.99 (1% decay):
  â†’ Proportional reduction â€” popular queries lose more points, but stay popular
  â†’ "diwali" at 10M loses 100K; "rare query" at 1K loses 10

Subtract fixed amount (e.g., 1000):
  â†’ Acts as a minimum activity floor
  â†’ Queries with low activity eventually hit zero and get cleaned up

Together: recent popular queries rise, old stale queries decay and vanish âœ…
```

---

## ğŸ–±ï¸ Debouncing on the Client

### The Problem

A fast typist can type 10 characters in under a second:

```
Without debouncing:
  User types "diwali" quickly (6 chars in 300ms)
  â†’ 6 API requests fired to server
  â†’ Server processes all 6 simultaneously
  â†’ Only the last one matters!
  â†’ Wasteful: 5 out of 6 requests are irrelevant
```

### Solution: Debouncing

```
WITH DEBOUNCING (wait for 200ms pause in typing):

  t=0ms:   User types "d"    â†’ start timer (200ms)
  t=50ms:  User types "i"    â†’ reset timer (200ms)
  t=100ms: User types "w"    â†’ reset timer (200ms)
  t=150ms: User types "a"    â†’ reset timer (200ms)
  t=200ms: User types "l"    â†’ reset timer (200ms)
  t=250ms: User types "i"    â†’ reset timer (200ms)
  t=450ms: TIMER FIRES       â†’ send request for "diwali" âœ…

  Result: 1 API request instead of 6 (83% reduction!)
```

### Debounce + Min-Character Combined

```javascript
// Client-side pseudocode
let debounceTimer = null;

function onKeyPress(inputValue) {
  clearTimeout(debounceTimer);

  if (inputValue.length < 3) {
    clearSuggestions();  // less than 3 chars â†’ no suggestions
    return;
  }

  debounceTimer = setTimeout(() => {
    fetchSuggestions(inputValue);  // fire API after 200ms pause
  }, 200);
}
```

| Technique | What it Does | Reduces Load By |
|-----------|-------------|-----------------|
| **Minimum 3 chars** | Don't fire for "d", "di" | ~30% |
| **Debouncing** | Only fire after a pause in typing | ~70-80% |
| **Combined** | Both applied together | ~85-90% |

---

# PART 9: SUMMARY & INTERVIEW PREP

---

## ğŸ“‹ Quick Reference Cheatsheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TYPEAHEAD SYSTEM â€” CHEATSHEET                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SCALE:                                                      â”‚
â”‚    10B searches/day â†’ 70B typeahead requests/day             â”‚
â”‚    ~800K Read QPS, ~100K Write QPS                           â”‚
â”‚    Data: ~100 bytes/query Ã— 1.5B unique = ~150 GB/day        â”‚
â”‚                                                              â”‚
â”‚  CAP CHOICE: AP System                                       â”‚
â”‚    Availability > Consistency (stale top-5 is fine)          â”‚
â”‚    Eventual consistency for rankings                         â”‚
â”‚                                                              â”‚
â”‚  LATENCY TARGET: < 10ms server-side                          â”‚
â”‚    Achieved via: Redis cache (95%+ hit rate)                 â”‚
â”‚                                                              â”‚
â”‚  DATA STRUCTURES:                                            â”‚
â”‚    Prefix Store   (KV): prefix â†’ top5 list                   â”‚
â”‚    HitCount Store (KV): full_query â†’ count                  â”‚
â”‚                                                              â”‚
â”‚  WRITE OPTIMIZATIONS:                                        â”‚
â”‚    1. Async via Kafka (decouple writes from reads)           â”‚
â”‚    2. Sampling (1/1000) â€” reduce write volume by 1000x       â”‚
â”‚    3. Skip-if-unchanged prefix updates                       â”‚
â”‚    4. Time decay â€” auto-remove stale queries                 â”‚
â”‚                                                              â”‚
â”‚  CLIENT OPTIMIZATIONS:                                       â”‚
â”‚    1. Min 3 characters to trigger typeahead                  â”‚
â”‚    2. Debouncing â€” wait 200ms after last keypress            â”‚
â”‚                                                              â”‚
â”‚  SHARDING: By prefix (prefix â†’ shard mapping)               â”‚
â”‚    All prefixes of a query go to the same shard âœ…           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Practice Questions & Solutions

### Conceptual Questions

**Q1.** What is the difference between typeahead and search results? Give a real-world example.

**Q2.** Why do we activate typeahead suggestions only after 3 characters are typed? What happens at 1 or 2 characters?

**Q3.** Should a typeahead system be CP or AP? Justify your answer with reasoning.

**Q4.** What is debouncing? Why is it critical for typeahead performance?

**Q5.** Explain the two-hashmap approach for typeahead. What does each hashmap store and when is each used?

**Q6.** Why is the Trie not ideal for a production-scale typeahead system like Google's?

**Q7.** What is "write amplification" in the context of typeahead? How do sampling and batching reduce it?

**Q8.** What is time decay? Why is it needed in a typeahead system? Give an example.

**Q9.** How would you shard a typeahead system? What is your sharding key and why?

**Q10.** If two users simultaneously search for "diwali" from different regions, they might see slightly different top-5 lists. Is this a problem? Why or why not?

---

### System Design Application Questions

**Q11.** Design the typeahead system for Amazon. How does it differ from Google's typeahead?

**Q12.** Your typeahead system's HIT COUNT STORE is getting overwhelmed. Walk through 3 techniques to reduce write load.

**Q13.** A new trending event (e.g., IPL final) causes a query to spike from 1,000 hits to 10,000,000 hits in one hour. How does your typeahead system pick this up and display it?

**Q14.** How would you handle a query that is suddenly trending negatively (e.g., a brand scandal where you want to suppress suggestions)? This is a content moderation question.

**Q15.** Estimate the storage needed for a typeahead system serving 500M DAU with 20 searches per user per day, each query averaging 10 characters.

---

## ğŸ“ Solutions

### Q1 â€” Typeahead vs Search Results

**Typeahead** shows *query suggestions* â€” it predicts what the user is about to type.
**Search Results** show *actual content* (pages, products, videos) matching the submitted query.

Example (Google):
- Typeahead: User types "diw" â†’ sees ["diwali", "diwali 2024", ...]
- Search Results: User hits Enter â†’ sees Wikipedia article on Diwali, news articles, images

These are **completely separate systems** with different architectures, databases, and latency requirements.

---

### Q2 â€” Why 3 Characters Minimum?

- At 1-2 characters, the prefix is too broad â†’ the suggestions would be irrelevant and noisy
  - Typing "d" â†’ suggestions: "download", "discord", "divorce", "dubai", "diwali" (all meaningless)
  - Typing "di" â†’ still very broad
  - Typing "diw" â†’ now clearly Diwali-related â†’ useful!
- Fewer characters = more queries to the server (every keystroke fires a request)
- 3-char minimum reduces unnecessary API calls and improves suggestion quality

---

### Q3 â€” CP vs AP for Typeahead

**Answer: AP (Available + Partition Tolerant)**

- If the system is down, users get NO suggestions â†’ terrible UX (like Google breaking entirely)
- If the ranking is 2% stale (shows "diwali wishes" instead of "diwali greetings"), users don't notice
- The cost of unavailability is much higher than the cost of slight inconsistency
- Therefore: prioritize **availability** and accept **eventual consistency** for rankings

---

### Q4 â€” Debouncing

Debouncing is waiting for a "pause" in typing before firing an API request. Without it, every keystroke fires a request â€” wasting server resources on incomplete prefixes the user will type past in milliseconds.

Example: User types "diwali" in 300ms without debouncing â†’ 6 API requests. With 200ms debounce â†’ typically 1-2 API requests.

---

### Q5 â€” Two-Hashmap Approach

**Hashmap 1 â€” PREFIX STORE:**
- Key: prefix string (e.g., "diw")
- Value: top 5 full queries for that prefix (e.g., ["diwali", "diwali 2024", ...])
- Read by: Typeahead Service on every keypress
- Optimized for: fast reads

**Hashmap 2 â€” HIT COUNT STORE:**
- Key: full query string (e.g., "diwali")
- Value: integer hit count
- Written by: Consumer Service when a search event is processed
- Optimized for: fast writes and upserts

They are separate to avoid read-write contention on the same data structure.

---

### Q7 â€” Write Amplification & Optimizations

**Write amplification** = one logical write (1 search event) causing many physical writes.

Example: Query "diwali rangoli designs" (23 chars) â†’ updates hit count + 20 prefix entries = 21 writes for 1 search.

**Solutions:**
1. **Sampling**: Only write every 1000th search â†’ 1000x fewer DB writes
2. **Skip-if-unchanged**: When recalculating top5 for a prefix, if the ordering didn't change, skip the write entirely
3. **Batching via Kafka**: Buffer many events and process them in bulk rather than one-by-one

---

### Q12 â€” HIT COUNT STORE Overwhelmed

Three techniques to reduce write load:

1. **Sampling:** Only update HIT COUNT STORE every N searches (N = 100 or 1000). The relative ranking stays accurate because all queries are sampled at the same rate.

2. **In-memory Buffering with Threshold:** Keep a local in-memory counter per query. Only flush to DB when the local counter reaches a threshold (e.g., 500). This batches 500 writes into 1.

3. **Time-windowed aggregation via Kafka:** Consume from Kafka in micro-batches (e.g., every 10 seconds). For each batch, aggregate all counts for the same query before writing once. Instead of 100K individual writes/batch â†’ far fewer aggregated writes.

---

### Q15 â€” Storage Estimation

```
Given:
  DAU = 500M
  Searches per user = 20/day
  Avg query length = 10 chars

Total searches/day    = 500M Ã— 20 = 10B
Unique queries (15%)  = 10B Ã— 0.15 = 1.5B unique queries/day
Storage per query     = 30 bytes (query) + 8 bytes (count) + overhead â‰ˆ 100 bytes

Storage per day       = 1.5B Ã— 100 bytes = 150 GB/day

Prefix entries (7x multiplier) = 150 GB Ã— 7 = 1.05 TB/day

Cumulative (1 year, with decay removing old entries):
  Active dataset â‰ˆ 5-10 TB (manageable with 10-20 shards)
```

---

## ğŸ“š References & Resources

### Lecture Reference
- **HLD Case Study 2 â€” Typeahead (Session 8)**
  - Instructor: Tarun Malhotra (Software Engineer at Google)
  - Platform: Scaler Academy HLD Module
  - [YouTube Lecture](https://www.youtube.com/watch?v=p6x8QdWA-NU)

### Academic & Technical References
- **Trie Data Structure** â€” Original concept by Edward Fredkin (1960)
  - Also known as "Prefix Tree" or "Digital Tree"
- **Consistent Hashing** â€” Karger et al. (1997) â€” for shard routing
- **Apache Kafka** â€” [kafka.apache.org](https://kafka.apache.org/) â€” for async write pipeline

### System Design Resources
- [Designing Data-Intensive Applications â€” Martin Kleppmann](https://dataintensive.net/)
  - Chapter 3: Storage and Retrieval
  - Chapter 11: Stream Processing (for Kafka patterns)
- [System Design Primer (GitHub)](https://github.com/donnemartin/system-design-primer)
- [ByteByteGo â€” Typeahead Design](https://blog.bytebytego.com/)

### Related Production Systems
- [Elasticsearch Prefix Search](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html)
- [Redis Sorted Sets for Top-K](https://redis.io/docs/data-types/sorted-sets/)
- [Google Suggest API (historical)](https://suggestqueries.google.com/complete/search)

---

> ğŸ“Œ **Key Takeaway:** Typeahead is a deceptively simple feature with immense engineering depth at scale.
> The core insight is: **optimize ruthlessly for reads** (sub-10ms latency), **relax consistency** (eventual is fine),
> and **reduce write amplification** (sampling + batching + decay).
> The two-hashmap approach elegantly separates read concerns from write concerns, enabling independent scaling of each path.
